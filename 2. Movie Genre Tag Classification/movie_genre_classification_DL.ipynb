{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"movie_genre_classification_DL.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMQFu3O99INbqXNPjz9ZENs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3a5e2fdd3ce245cd93349ac8e2bb39b9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_463b1ad1d52c47f0a2bda2b70c628f1c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_581ddcdbc595404991ce31c58c4ebb78","IPY_MODEL_ab1aeb4531a540e289942bd91234d2f1"]}},"463b1ad1d52c47f0a2bda2b70c628f1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"581ddcdbc595404991ce31c58c4ebb78":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2cd1756117564852a0de46e6ec4851f8","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1042301,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1042301,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a669b6b572b34fb4bc7c675885801695"}},"ab1aeb4531a540e289942bd91234d2f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b502d0da1f574f79b32f60180f7cdae0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.04M/1.04M [00:00&lt;00:00, 5.10MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_905d303dd4af4d86bed4cd9c499dea16"}},"2cd1756117564852a0de46e6ec4851f8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a669b6b572b34fb4bc7c675885801695":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b502d0da1f574f79b32f60180f7cdae0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"905d303dd4af4d86bed4cd9c499dea16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a698716bbdff40d1a1908f217883ccef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7a3302f6e1854b249392afffa20242dd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5b68d8cb6b8347dd9dde0c9d005e32c1","IPY_MODEL_84793bc245744b7f8f8e168c751b7a23"]}},"7a3302f6e1854b249392afffa20242dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5b68d8cb6b8347dd9dde0c9d005e32c1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5afad4ddc802489fbafaa0afbd90f575","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":456318,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":456318,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e963c514febd436a922acb4d1abfaa6a"}},"84793bc245744b7f8f8e168c751b7a23":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6da69fbff324478f962721096e3d966d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 456k/456k [00:00&lt;00:00, 4.48MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c369a4266233412c858d05f19604a558"}},"5afad4ddc802489fbafaa0afbd90f575":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e963c514febd436a922acb4d1abfaa6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6da69fbff324478f962721096e3d966d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c369a4266233412c858d05f19604a558":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"eaef204b16dc48bfa380c3474359d830":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c8fc0befc76c493f875f5da2f1a9515e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_138ac0c23dc5436fa5650dd0c3265720","IPY_MODEL_c5813404da134772aad223bbaa7fb0ce"]}},"c8fc0befc76c493f875f5da2f1a9515e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"138ac0c23dc5436fa5650dd0c3265720":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8a0ebf1ad7a84da5ad1528ccc624c919","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":665,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":665,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8f7db27016394c5cb37863273d060e0f"}},"c5813404da134772aad223bbaa7fb0ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4b0dad9231694761b563fd220a0af90e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 665/665 [00:07&lt;00:00, 87.0B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_19fba4b8799b4ef0b2b9a7aeb91a8830"}},"8a0ebf1ad7a84da5ad1528ccc624c919":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8f7db27016394c5cb37863273d060e0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4b0dad9231694761b563fd220a0af90e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"19fba4b8799b4ef0b2b9a7aeb91a8830":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"04c7aa6daf844e9fbc7f9836d65daf65":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9041e0ae33434b34b0ca3a61b7f03ffd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7aa2ab62ffd54b8abcfe61f561bb4ee0","IPY_MODEL_af8f300bfce7463c9aab5f6234f29988"]}},"9041e0ae33434b34b0ca3a61b7f03ffd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7aa2ab62ffd54b8abcfe61f561bb4ee0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_714f6b9689ac430286fc1fa44034d8d7","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":548118077,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":548118077,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_118e48a1bda9406daea3496c50f39ff5"}},"af8f300bfce7463c9aab5f6234f29988":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_824e27c143c14997b971abab9e8f34f1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 548M/548M [00:07&lt;00:00, 77.3MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a9089787b0f24dd4bba183b71f0fc4f3"}},"714f6b9689ac430286fc1fa44034d8d7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"118e48a1bda9406daea3496c50f39ff5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"824e27c143c14997b971abab9e8f34f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a9089787b0f24dd4bba183b71f0fc4f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"vyO-oNMNj0KK"},"source":["This notebook demonstrates the implementation of deep learning models to classify genre of a given set of movies."]},{"cell_type":"markdown","metadata":{"id":"OUzfTYw6ImOL"},"source":["Training Dataset: train.csv\n","\n","Test Dataset: test.csv\n","\n","To make life easy, ​word embeddings are already extracted using BERT \n","\n","The goal is to train 3 different deep learning models far as this task is concerned,<br><br>\n","- Model1: Train at least 1 CNN on the BERT embeddings<br>\n","- Model2: Train any model of your choice on the BERT embeddings\n","- Model3: Train any other model of your choice + different embeddings other than BERT\n","\n","and evaluate the predictions on the test dataset\n"]},{"cell_type":"markdown","metadata":{"id":"FyXDcpPKH6YF"},"source":["### Accessing Google Drive from Google Colab"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r2bJfBKOHsD3","executionInfo":{"status":"ok","timestamp":1611570525767,"user_tz":-60,"elapsed":2120,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"c7588f92-75cf-41ea-c327-ab6181fcefb1"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LSou2g4PjJop"},"source":["Setting path variables for easier imports in google colab"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5s5Ia5QUm2Jq","executionInfo":{"status":"ok","timestamp":1611570526076,"user_tz":-60,"elapsed":1946,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"bd63337c-fd55-4365-83e7-d1df22d38e45"},"source":["%cd /content/gdrive/MyDrive/colab_notebooks/genre_classification/\n","import sys\n","sys.path.insert(0,'/content/gdrive/MyDrive/colab_notebooks/genre_classification')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive/colab_notebooks/genre_classification\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ozKDvIQ4IhR2"},"source":["### Install, Import required libraries"]},{"cell_type":"markdown","metadata":{"id":"As1jRzzCJu0c"},"source":["If you are working in google colab, run this cell, else run the following one"]},{"cell_type":"code","metadata":{"id":"VlnWgiVcJkVZ","executionInfo":{"status":"ok","timestamp":1611570528575,"user_tz":-60,"elapsed":3868,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}}},"source":["# Install Transformer library for models and tokenizer\n","\n","!pip install -U -q transformers "],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"rS5nNbm7m-yf","executionInfo":{"status":"ok","timestamp":1611570530338,"user_tz":-60,"elapsed":5366,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}}},"source":["# Importing the required libraries\n","import numpy as np\n","import pandas as pd\n","\n","from tqdm import tqdm\n","\n","import transformers\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader\n","\n","from sklearn.preprocessing import MultiLabelBinarizer"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L6e7UZ7GKGto"},"source":["### Load, Read the Data"]},{"cell_type":"code","metadata":{"id":"iW3RGk0yKLhO","executionInfo":{"status":"ok","timestamp":1611570530344,"user_tz":-60,"elapsed":3536,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}}},"source":["# Read data from .csv file\n","train_df = pd.read_csv('data/train.csv')\n","test_df = pd.read_csv('data/test.csv')"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jVhE5Up_LTb_","executionInfo":{"status":"ok","timestamp":1611570530345,"user_tz":-60,"elapsed":3201,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"b553da3c-7a42-432a-bcf3-592e6c3e24f3"},"source":["# Print the shape of train and test dataframe(s)\n","print(\"Training dataset has {} rows and {} columns\".format(train_df.shape[0], train_df.shape[1]))\n","print(\"Test dataset has {} rows and {} columns\".format(test_df.shape[0], test_df.shape[1]))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Training dataset has 3054 rows and 8 columns\n","Test dataset has 3054 rows and 7 columns\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":272},"id":"Y5ikHJPDOf1m","executionInfo":{"status":"ok","timestamp":1611570530348,"user_tz":-60,"elapsed":2917,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"59bc57be-7948-4c2c-ec10-b2d589e9ee42"},"source":["# Display first 5 rows of training dataset\n","train_df.head()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>type</th>\n","      <th>title</th>\n","      <th>director</th>\n","      <th>cast</th>\n","      <th>country</th>\n","      <th>rating</th>\n","      <th>description</th>\n","      <th>genres</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Movie</td>\n","      <td>The Ryan White Story</td>\n","      <td>John Herzfeld</td>\n","      <td>Judith Light, Lukas Haas, Michael Bowen, Nikki...</td>\n","      <td>United States</td>\n","      <td>TV-PG</td>\n","      <td>After contracting HIV from a tainted blood tre...</td>\n","      <td>Drama</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Movie</td>\n","      <td>Mumbai Cha Raja</td>\n","      <td>Manjeet Singh</td>\n","      <td>Rahul Bairagi, Arbaaz Khan, Tejas Parvatkar, D...</td>\n","      <td>India</td>\n","      <td>TV-14</td>\n","      <td>This coming-of-age tale follows Rahul, a young...</td>\n","      <td>Drama, International</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Movie</td>\n","      <td>Soekarno</td>\n","      <td>Hanung Bramantyo</td>\n","      <td>Ario Bayu, Lukman Sardi, Maudy Koesnaedi, Tant...</td>\n","      <td>Indonesia</td>\n","      <td>TV-MA</td>\n","      <td>This biographical drama about Indonesia's firs...</td>\n","      <td>Drama, International</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Movie</td>\n","      <td>The Young Offenders</td>\n","      <td>Peter Foott</td>\n","      <td>Alex Murphy, Chris Walley, Hilary Rose, Domini...</td>\n","      <td>Ireland</td>\n","      <td>TV-MA</td>\n","      <td>Never ones to think things through, two Irish ...</td>\n","      <td>Comedy, International</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Movie</td>\n","      <td>The King</td>\n","      <td>David Michôd</td>\n","      <td>Timothée Chalamet, Joel Edgerton, Robert Patti...</td>\n","      <td>NaN</td>\n","      <td>R</td>\n","      <td>Wayward Prince Hal must turn from carouser to ...</td>\n","      <td>Drama</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    type  ...                 genres\n","0  Movie  ...                  Drama\n","1  Movie  ...   Drama, International\n","2  Movie  ...   Drama, International\n","3  Movie  ...  Comedy, International\n","4  Movie  ...                  Drama\n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":289},"id":"bxtHFsv4Ovcw","executionInfo":{"status":"ok","timestamp":1611570715111,"user_tz":-60,"elapsed":545,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"f066efee-0808-4e65-c1af-f0eba9fa8b26"},"source":["# Display first 5 rows of test dataset\n","test_df.head()"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>type</th>\n","      <th>title</th>\n","      <th>director</th>\n","      <th>cast</th>\n","      <th>country</th>\n","      <th>rating</th>\n","      <th>description</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Movie</td>\n","      <td>The Bill Murray Stories: Life Lessons Learned ...</td>\n","      <td>Tommy Avallone</td>\n","      <td>Tommy Avallone, Bill Murray, Joel Murray, Pete...</td>\n","      <td>United States</td>\n","      <td>TV-MA</td>\n","      <td>This documentary highlights spontaneous encoun...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Movie</td>\n","      <td>The Short Game</td>\n","      <td>Josh Greenbaum</td>\n","      <td>Sky Sudberry, Allan Kournikova, Jed Dy, Zamoku...</td>\n","      <td>United States</td>\n","      <td>PG</td>\n","      <td>They are fiercely competitive athletes, determ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Movie</td>\n","      <td>The Bad Batch</td>\n","      <td>Ana Lily Amirpour</td>\n","      <td>Suki Waterhouse, Jason Momoa, Keanu Reeves, Ji...</td>\n","      <td>United States</td>\n","      <td>R</td>\n","      <td>Banished to a wasteland of undesirables, a you...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>TV Show</td>\n","      <td>The Twilight Zone (Original Series)</td>\n","      <td>NaN</td>\n","      <td>Rod Serling</td>\n","      <td>United States</td>\n","      <td>TV-PG</td>\n","      <td>Hosted by creator Rod Serling, this groundbrea...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Movie</td>\n","      <td>World Trade Center</td>\n","      <td>Oliver Stone</td>\n","      <td>Nicolas Cage, Michael Peña, Maggie Gyllenhaal,...</td>\n","      <td>United States</td>\n","      <td>PG-13</td>\n","      <td>Working under treacherous conditions, an army ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      type  ...                                        description\n","0    Movie  ...  This documentary highlights spontaneous encoun...\n","1    Movie  ...  They are fiercely competitive athletes, determ...\n","2    Movie  ...  Banished to a wasteland of undesirables, a you...\n","3  TV Show  ...  Hosted by creator Rod Serling, this groundbrea...\n","4    Movie  ...  Working under treacherous conditions, an army ...\n","\n","[5 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x2oFB6PKtmGx","executionInfo":{"status":"ok","timestamp":1611570530384,"user_tz":-60,"elapsed":1528,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"192f53d3-ecbe-4d96-8c1e-9170e887b26c"},"source":["# Features of training dataframe\n","train_df.info()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 3054 entries, 0 to 3053\n","Data columns (total 8 columns):\n"," #   Column       Non-Null Count  Dtype \n","---  ------       --------------  ----- \n"," 0   type         3054 non-null   object\n"," 1   title        3054 non-null   object\n"," 2   director     2112 non-null   object\n"," 3   cast         2767 non-null   object\n"," 4   country      2838 non-null   object\n"," 5   rating       3053 non-null   object\n"," 6   description  3054 non-null   object\n"," 7   genres       3054 non-null   object\n","dtypes: object(8)\n","memory usage: 191.0+ KB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_RPUwCRXt1cP","executionInfo":{"status":"ok","timestamp":1611570530385,"user_tz":-60,"elapsed":1302,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"df777e74-16a9-4f3b-c4bd-bb0ca60bab97"},"source":["# Features of test dataframe\n","test_df.info()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 3054 entries, 0 to 3053\n","Data columns (total 7 columns):\n"," #   Column       Non-Null Count  Dtype \n","---  ------       --------------  ----- \n"," 0   type         3054 non-null   object\n"," 1   title        3054 non-null   object\n"," 2   director     2106 non-null   object\n"," 3   cast         2791 non-null   object\n"," 4   country      2831 non-null   object\n"," 5   rating       3052 non-null   object\n"," 6   description  3054 non-null   object\n","dtypes: object(7)\n","memory usage: 167.1+ KB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UNTOZ19xKCtV"},"source":["- For predicting genre, we will take into account `title` and `description` features only for obvious reasons and these don't have any missing values records/observations.\n","- Based on the nature of the dataset, this is a multi-label classification task\n","- Features: `title`, `description`\n","- Label: `genres`"]},{"cell_type":"markdown","metadata":{"id":"ztMzS6h4xXvW"},"source":["We will now segregate `genres` for every observations followed by converting them into one-hot encoded matrices"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AsfrHt4Kvulp","executionInfo":{"status":"ok","timestamp":1611570531826,"user_tz":-60,"elapsed":920,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"cda56639-ea63-4c24-a1d1-ea28db5e5118"},"source":["# Split individual genres\n","train_df[\"genres\"] = train_df.genres.apply(lambda x: [i.strip() for i in x.split(\",\")])\n","\n","# Initialize multi-label transformer\n","mlb = MultiLabelBinarizer()\n","\n","# Create a new dataframe 'ohe_labels' having one-hot encoded representation\n","ohe_labels = pd.DataFrame(mlb.fit_transform(train_df.genres), columns=mlb.classes_)\n","\n","# Merge 'train_df' and 'ohe_labels'\n","train_df = pd.concat((train_df, ohe_labels), axis=1)\n","\n","# Display the different genres available\n","print(\"These are the labels exising in the dataset\\n\", mlb.classes_)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["These are the labels exising in the dataset\n"," ['Action' 'Anime' 'Comedy' 'Documentaries' 'Drama' 'Horror'\n"," 'International' 'Kids' 'Romantic' 'Sci-Fi & Fantasy' 'Stand-up'\n"," 'Thriller']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":369},"id":"sok3b48Xve48","executionInfo":{"status":"ok","timestamp":1611570532109,"user_tz":-60,"elapsed":912,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"b6db4499-ad55-4f5e-eb20-91f1a3d5f720"},"source":["# Display first two rows of 'train_df' post transformation\n","train_df.head(2)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>type</th>\n","      <th>title</th>\n","      <th>director</th>\n","      <th>cast</th>\n","      <th>country</th>\n","      <th>rating</th>\n","      <th>description</th>\n","      <th>genres</th>\n","      <th>Action</th>\n","      <th>Anime</th>\n","      <th>Comedy</th>\n","      <th>Documentaries</th>\n","      <th>Drama</th>\n","      <th>Horror</th>\n","      <th>International</th>\n","      <th>Kids</th>\n","      <th>Romantic</th>\n","      <th>Sci-Fi &amp; Fantasy</th>\n","      <th>Stand-up</th>\n","      <th>Thriller</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Movie</td>\n","      <td>The Ryan White Story</td>\n","      <td>John Herzfeld</td>\n","      <td>Judith Light, Lukas Haas, Michael Bowen, Nikki...</td>\n","      <td>United States</td>\n","      <td>TV-PG</td>\n","      <td>After contracting HIV from a tainted blood tre...</td>\n","      <td>[Drama]</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Movie</td>\n","      <td>Mumbai Cha Raja</td>\n","      <td>Manjeet Singh</td>\n","      <td>Rahul Bairagi, Arbaaz Khan, Tejas Parvatkar, D...</td>\n","      <td>India</td>\n","      <td>TV-14</td>\n","      <td>This coming-of-age tale follows Rahul, a young...</td>\n","      <td>[Drama, International]</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    type                 title  ... Stand-up Thriller\n","0  Movie  The Ryan White Story  ...        0        0\n","1  Movie       Mumbai Cha Raja  ...        0        0\n","\n","[2 rows x 20 columns]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"kaXdAHV01yIq"},"source":["### Model 1: CNN Model with BERT pre-trained tokenizers"]},{"cell_type":"markdown","metadata":{"id":"m4nOAMVP2dWN"},"source":["**Define the Architecture**"]},{"cell_type":"code","metadata":{"id":"crWzLS6YS5VB","executionInfo":{"status":"ok","timestamp":1611570533996,"user_tz":-60,"elapsed":509,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}}},"source":["class simpleCNN(nn.Module):\n","    \"\"\"Model that uses CNN and with pretrained-model- distil-bert model\n","     ARGS: \n","          embed_dim: embedding dimension is 768 for BERT models\n","          class_num: total number of labels\n","          kernel_num: Number of kernels in cnn\n","          kernel_size: list of kernels of sizes for different kernel\n","          dropout: dropout to be used for regularization \n","     Attributes:\n","          bert_model: BERT model with pretrianed weights for transfer learning\n","          convs1: list of cnn model with respective kernel size and dimensions\n","          fc: final linear/dense layer with class_num output\n","     Abreviations:\n","     N: Batch size\n","     Ci: input Channel size, it is 1 here \n","     W: words size\n","     D: embedding size\n","     Co: output channel  \n","     Ks: Kernel size\n","     C: classes\n","    \"\"\"\n","    def __init__(self, embed_dim, class_num, kernel_num, kernel_sizes, dropout):\n","        super(simpleCNN, self).__init__()\n","        self.bert_model = transformers.BertModel.from_pretrained(pretrained_weights)\n","        #pytorch uses special-list comprehension dedicated to its model classes\n","        self.convs1 = nn.ModuleList([nn.Conv2d(1, kernel_num, (K, embed_dim)) for K in kernel_sizes])\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc1 = nn.Linear(len(kernel_sizes) * kernel_num, class_num) \n","        \n","    def forward(self, ids,att,token):\n","        x_ = self.bert_model(ids,att,token)[0] \n","        x = x_.unsqueeze(1)  # (N, Ci, W, D) \n","        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks) \n","        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n","        x = torch.cat(x, 1)\n","        x = self.dropout(x)  # (N, len(Ks)*Co)\n","        logit = self.fc1(x)  # (N, C)\n","        return logit "],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"coUOqQNJ4v5v"},"source":["**Dataset Module** "]},{"cell_type":"code","metadata":{"id":"2Y7xpEf3TNtV","executionInfo":{"status":"ok","timestamp":1611570538304,"user_tz":-60,"elapsed":476,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}}},"source":["from torch.utils.data import Dataset,DataLoader\n","tokenizer_class = transformers.BertTokenizer\n","pretrained_weights='distilbert-base-uncased'\n","class GenreDataset(Dataset):\n","  \"\"\"Dataset class with pretrained-embeddings from the distil-bert model.\n","     ARGS: \n","          description: total list of text description \n","          title: total list of titles\n","          labels: one-hot encoded labels \n","     Attributes:\n","          tokenizer: Tokenizing the text and embedding with ids.\n","          max_seq: maximum number of word to consider and truncate \n","                   if there are more or pad with [0] they are less.\n","          \n","     Abreviations:\n","     N: Batch size\n","     Ci: input Channel size, it is 1 here \n","     W: words size\n","     D: embedding size\n","     Co: output channel  \n","     Ks: Kernel size\n","     C: classes\n","  \"\"\"\n","  def __init__(self,description,title,labels):\n","    self.title = title\n","    self.description = description\n","    self.labels = labels \n","    self.max_seq = 250 #shouldn't be > 512 for BERT\n","    self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n","  \n","  def __len__(self):\n","    return len(self.title)\n","  \n","  def __getitem__(self,idx): \n","    #convert each text input to string without gaps and join them before tokenizing\n","    title = \"\".join(self.title[idx].split())\n","    description = \"\".join(self.description[idx].split()) \n","    labels = self.labels[idx,:]\n","    inputs = self.tokenizer(title + description, add_special_tokens=True,truncation=True,max_length=self.max_seq)\n","    # here 'input_ids' implies the token numbers given by the embeddings\n","    input_ids = inputs[\"input_ids\"]\n","    # 'token_type_ids' usually helpful if we are using separate two text data rather than single text data \n","    token_type_ids = inputs[\"token_type_ids\"]\n","    # 'attention_mask' will have 1:attending word and 0:padded word\n","    attention_mask = inputs[\"attention_mask\"]\n","    #here padding with [0] if the tokens are less than the max_seq\n","    input_ids = input_ids + [0] * (self.max_seq - len(input_ids))\n","    token_type_ids = token_type_ids + [0] * (self.max_seq - len(token_type_ids))\n","    attention_mask = attention_mask + [0] * (self.max_seq - len(attention_mask))\n","    return {\n","        \"input_ids\": torch.tensor(input_ids,dtype=torch.long),\n","        \"token_type_ids\": torch.tensor(token_type_ids,dtype=torch.long),\n","        \"attention_mask\": torch.tensor(attention_mask,dtype=torch.long),\n","        \"labels\": torch.tensor(labels,dtype=torch.float)\n","    } \n","\n"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6_cpTnmI5T9P"},"source":["**Defining Hyperparameters**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zDE29d0wTQKM","executionInfo":{"status":"ok","timestamp":1611570542972,"user_tz":-60,"elapsed":2671,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"119665fd-eb9b-4c44-9cc0-d5ab7a3d4674"},"source":["# Instantiating the cnn-bert model with arguments \n","embed_dim = 768\n","class_num = len(mlb.classes_)\n","kernel_num = 3\n","kernel_sizes = [2, 3, 4]\n","dropout = 0.5\n","\n","# Post running this cell model will be downloaded from web (ignore errors)\n","model = simpleCNN(\n","    embed_dim=embed_dim,\n","    class_num=class_num,\n","    kernel_num=kernel_num,\n","    kernel_sizes=kernel_sizes,\n","    dropout=dropout\n",")"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertModel: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"WZA2g-OjIlH7"},"source":["**Define train, val, test functions**"]},{"cell_type":"code","metadata":{"id":"hoSJ2HlHTTdE","executionInfo":{"status":"ok","timestamp":1611570551078,"user_tz":-60,"elapsed":523,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}}},"source":["\n","# Training function \n","def train_fn(dataloader,model,optimizer,device):\n","  # setting the supplied to training mode \n","  model.train() \n","  losses = []\n","  f_output = []\n","  f_target = [] \n","  # loading the batchwise data to the model \n","  for d in dataloader:\n","    in_ids,token_ids,att_mask,targets = d[\"input_ids\"].to(device),d[\"token_type_ids\"].to(device),d[\"attention_mask\"].to(device),d[\"labels\"].to(device)\n","    # setting optimizer to to zero \n","    optimizer.zero_grad()\n","    # forward propogation \n","    outs = model(in_ids,token_ids,att_mask)\n","    loss = loss_fn(outs,targets)\n","    # backpropogation \n","    loss.backward()\n","    # weights update \n","    optimizer.step()\n","    # getting loss,targets,output values as return \n","    losses.append(loss.cpu().detach())\n","    # final layer values with sigmoid to get values between 0 and 1\n","    outs = torch.sigmoid(outs)\n","    f_output.extend(outs.cpu().detach().numpy())\n","    f_target.extend(targets.cpu().detach().numpy())\n","  return f_output,f_target,np.sum(losses)/len(dataloader) \n","\n","def validation_fn(dataloader,model,device):\n","    model.eval()\n","    losses = []\n","    f_output = []\n","    f_target = []\n","    # keeping the no-gradient on  \n","    with torch.no_grad():\n","      # loading the batchwise data to the model\n","      for d in dataloader:\n","        in_ids,token_ids,att_mask,targets = d[\"input_ids\"].to(device),d[\"token_type_ids\"].to(device),d[\"attention_mask\"].to(device),d[\"labels\"].to(device)\n","        # forward propogation \n","        outs = model(in_ids,token_ids,att_mask)\n","        loss = loss_fn(outs,targets)\n","        # final layer with sigmoid to get values between 0 and 1\n","        outs = torch.sigmoid(outs)\n","        # getting loss,targets,output values as return \n","        losses.append(loss.cpu().detach())\n","        f_output.extend(outs.cpu().detach().numpy())\n","        f_target.extend(targets.cpu().detach().numpy())\n","    return f_output,f_target,np.sum(losses)/len(dataloader) \n","\n","def test_fn(dataloader,model,device):\n","    model.eval()\n","    f_output = []\n","    # keeping the no-gradient on  \n","    with torch.no_grad():\n","      # loading the batchwise data to the model\n","      for d in dataloader:\n","        in_ids,token_ids,att_mask = d[\"input_ids\"].to(device),d[\"token_type_ids\"].to(device),d[\"attention_mask\"].to(device)\n","        # forward propogation \n","        outs = model(in_ids,token_ids,att_mask)\n","        # final layer with sigmoid to get values between 0 and 1\n","        outs = torch.sigmoid(outs)\n","        # getting loss,targets,output values as return \n","        f_output.extend(outs.cpu().detach().numpy())\n","    return f_output \n","\n","# This loss combines a Sigmoid layer and the BCELoss in one single class. \n","def loss_fn(out,target):\n","  return nn.BCEWithLogitsLoss()(out,target)"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YPHC7paDFG8c"},"source":["**Train Model**"]},{"cell_type":"code","metadata":{"id":"OyOggOnDTiri","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611561520620,"user_tz":-60,"elapsed":1365354,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"245ab829-7de6-4641-d13a-4228e45b1299"},"source":["# define number of epochs\r\n","epochs = 10\r\n","from sklearn.model_selection import train_test_split\r\n","from eval_metric import evaluate_results\r\n","from sklearn.metrics import accuracy_score\r\n","\r\n","# splitting the train and test dataset \r\n","train,val = train_test_split(train_df,test_size=0.1)\r\n","\r\n","# instantiating dataset class with supplied data and labels \r\n","t_dataset = GenreDataset(train[\"description\"].values,train[\"title\"].values,train[mlb.classes_].values)\r\n","# dataset and batchwise loader from pytorch \r\n","t_loader = DataLoader(t_dataset,batch_size=32,shuffle=True,num_workers=4)\r\n","\r\n","v_dataset = GenreDataset(val[\"description\"].values,val[\"title\"].values,val[mlb.classes_].values)\r\n","v_loader = DataLoader(v_dataset,batch_size=32,shuffle=False,num_workers=4)\r\n","\r\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n","#optimizer methodology used with very low learning rate \r\n","optimizer = torch.optim.Adam(model.parameters(),lr=1e-5)\r\n","# loading the model to device for getting the weights to device \r\n","model.to(device)\r\n","best_loss = np.inf \r\n","print(\"TRAINING STARTED...\") \r\n","\r\n","for e in range(epochs):\r\n","  t_out,t_target,train_loss = train_fn(t_loader,model,optimizer,device)\r\n","  v_out,v_target,val_loss = validation_fn(v_loader,model,device)  \r\n","  # here evaluation results from the given file \r\n","  acc,f1,fpr,fnr = evaluate_results(np.array(t_target),np.round(t_out))\r\n","  acc2,f1_2,fpr2,fnr2 = evaluate_results(np.array(v_target),np.round(v_out))\r\n","  print(f\"{e+1}-\")\r\n","  if val_loss < best_loss:\r\n","    torch.save(model.state_dict(),\"model_bert.pth\")\r\n","    best_loss = val_loss \r\n","  # uncomment below code to get the accuracy score directly from sklearn \r\n","  #print((accuracy_score(np.array(t_target),np.round(t_out)),accuracy_score(np.array(v_target),np.round(v_out))))\r\n","\r\n","  print(\"train_loss:\", round(train_loss,3),\"train_accuracy:\",round(acc,3), \"train_f1:\", f1, \"train_fpr:\", fpr, \"train_fnr:\", fnr) \r\n","  print(\"val_loss:\",round(val_loss,3),\"val_accuracy:\",round(acc2,3), \"val_f1:\", f1_2, \"val_fpr:\", fpr2, \"val_fnr:\", fnr2)\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TRAINING STARTED...\n","1-\n","train_loss: 0.6 train_accuracy: 0.003 train_f1: [0.1384820239680426, 0.0588235294117647, 0.26220735785953175, 0.18352941176470586, 0.18521284540702015, 0.07969151670951158, 0.2624384909786769, 0.0, 0.0, 0.0, 0.10328638497652581, 0.10964083175803402] train_fpr: [0.15713698066639242, 0.26905829596412556, 0.28865979381443296, 0.3155402496771416, 0.12450028555111364, 0.23596792668957617, 0.14904552129221732, 0.0012432656444260257, 0.0, 0.0, 0.4085483249903735, 0.7083820662768031] train_fnr: [0.8359621451104101, 0.6666666666666666, 0.7243319268635724, 0.7247058823529412, 0.8756268806419257, 0.7596899224806202, 0.8268398268398268, 1.0, 1.0, 1.0, 0.5629139072847682, 0.366120218579235]\n","val_loss: 0.572 val_accuracy: 0.003 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1398176291793313] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]\n","2-\n","train_loss: 0.603 train_accuracy: 0.004 train_f1: [0.12396694214876032, 0.03792667509481669, 0.25623268698060947, 0.1914241960183767, 0.1689497716894977, 0.08820023837902265, 0.27438370846730975, 0.0, 0.0, 0.0, 0.09728867623604466, 0.12067352666043031] train_fpr: [0.1497326203208556, 0.26307922272047835, 0.2690230731467845, 0.3254412397761515, 0.11764705882352941, 0.25696830851470026, 0.1644640234948605, 0.0, 0.0, 0.0, 0.4012321909896034, 0.7118908382066277] train_fnr: [0.8580441640378549, 0.7916666666666666, 0.739803094233474, 0.7058823529411765, 0.8886659979939819, 0.7131782945736435, 0.8152958152958153, 1.0, 1.0, 1.0, 0.5960264900662252, 0.29508196721311475]\n","val_loss: 0.568 val_accuracy: 0.003 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1398176291793313] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]\n","3-\n","train_loss: 0.602 train_accuracy: 0.005 train_f1: [0.12058823529411765, 0.03579952267303103, 0.27232796486090777, 0.21161495624502782, 0.14726840855106887, 0.09466019417475727, 0.21135469364811696, 0.0, 0.0, 0.0, 0.0912667191188041, 0.12948960302457468] train_fpr: [0.13245577951460305, 0.2806427503736921, 0.23024054982817868, 0.3009040034438226, 0.09880068532267276, 0.25047728140511644, 0.15051395007342144, 0.0, 0.0, 0.0, 0.4089333846746246, 0.7001949317738791] train_fnr: [0.8706624605678234, 0.7916666666666666, 0.7383966244725738, 0.6870588235294117, 0.9067201604814443, 0.6976744186046512, 0.8643578643578643, 1.0, 1.0, 1.0, 0.6158940397350994, 0.25136612021857924]\n","val_loss: 0.563 val_accuracy: 0.003 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1398176291793313] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]\n","4-\n","train_loss: 0.598 train_accuracy: 0.003 train_f1: [0.14804469273743015, 0.04962779156327543, 0.26480836236933797, 0.20190779014308427, 0.21895664952241, 0.07187112763320942, 0.21843003412969283, 0.0, 0.0, 0.0, 0.08007117437722419, 0.1240166589541879] train_fpr: [0.1423282599753188, 0.26681614349775784, 0.2621502209131075, 0.3039173482565648, 0.12278697886921759, 0.2478045055364643, 0.13215859030837004, 0.0, 0.0, 0.0, 0.3573353869849827, 0.7189083820662768] train_fnr: [0.832807570977918, 0.7222222222222222, 0.7327707454289732, 0.7011764705882353, 0.8505516549648947, 0.7751937984496124, 0.8614718614718615, 1.0, 1.0, 1.0, 0.7019867549668874, 0.2677595628415301]\n","val_loss: 0.568 val_accuracy: 0.003 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1398176291793313] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]\n","5-\n","train_loss: 0.597 train_accuracy: 0.003 train_f1: [0.11611030478955006, 0.05499999999999999, 0.2642857142857143, 0.20928402832415421, 0.17108251324753973, 0.07701283547257876, 0.2244668911335578, 0.0, 0.0, 0.0, 0.07672188317349607, 0.12921615201900238] train_fpr: [0.13656931303990127, 0.26382660687593423, 0.24742268041237114, 0.3069306930693069, 0.12050256996002284, 0.2653684612447499, 0.14390602055800295, 0.0, 0.0, 0.0, 0.3665768194070081, 0.6962962962962963] train_fnr: [0.8738170347003155, 0.6944444444444444, 0.739803094233474, 0.6870588235294117, 0.8866599799398195, 0.7441860465116279, 0.8556998556998557, 1.0, 1.0, 1.0, 0.7086092715231788, 0.2568306010928962]\n","val_loss: 0.559 val_accuracy: 0.003 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1398176291793313] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]\n","6-\n","train_loss: 0.599 train_accuracy: 0.004 train_f1: [0.1476121562952243, 0.048109965635738834, 0.2524682651622003, 0.18168168168168167, 0.15963855421686748, 0.08439897698209718, 0.1994236311239193, 0.0, 0.0, 0.0, 0.087248322147651, 0.11683168316831682] train_fpr: [0.13286713286713286, 0.2914798206278027, 0.25920471281296026, 0.33835557468790356, 0.12849800114220444, 0.2367315769377625, 0.12922173274596183, 0.0, 0.0, 0.0, 0.38082402772429724, 0.6701754385964912] train_fnr: [0.8391167192429022, 0.7083333333333334, 0.7482419127988749, 0.7152941176470589, 0.8936810431293881, 0.7441860465116279, 0.8751803751803752, 1.0, 1.0, 1.0, 0.6556291390728477, 0.3551912568306011]\n","val_loss: 0.568 val_accuracy: 0.003 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1398176291793313] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]\n","7-\n","train_loss: 0.597 train_accuracy: 0.003 train_f1: [0.14388489208633096, 0.04970414201183433, 0.2608089260808926, 0.20722135007849296, 0.17134192570128884, 0.06097560975609756, 0.210880538418396, 0.0, 0.0, 0.0, 0.09632224168126094, 0.12147716229348882] train_fpr: [0.134923899629782, 0.28101644245142005, 0.26313205694648995, 0.3086526043908739, 0.11936036550542548, 0.2542955326460481, 0.15345080763582966, 0.0, 0.0, 0.0, 0.36041586445899115, 0.682261208576998] train_fnr: [0.8422712933753943, 0.7083333333333334, 0.7369901547116737, 0.6894117647058824, 0.8866599799398195, 0.8062015503875969, 0.8643578643578643, 1.0, 1.0, 1.0, 0.6357615894039735, 0.31693989071038253]\n","val_loss: 0.565 val_accuracy: 0.003 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1398176291793313] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]\n","8-\n","train_loss: 0.592 train_accuracy: 0.004 train_f1: [0.1343065693430657, 0.05815423514538559, 0.28223495702005735, 0.1903914590747331, 0.1841317365269461, 0.08292682926829269, 0.22777777777777777, 0.0, 0.0, 0.0, 0.08588957055214724, 0.12185297079556896] train_fpr: [0.13245577951460305, 0.2600896860986547, 0.23956799214531174, 0.254842875591907, 0.12335808109651628, 0.2508591065292096, 0.15345080763582966, 0.0, 0.0, 0.0, 0.3623411628802464, 0.6557504873294348] train_fnr: [0.8548895899053628, 0.6805555555555556, 0.7229254571026723, 0.7482352941176471, 0.876629889669007, 0.7364341085271318, 0.8520923520923521, 1.0, 1.0, 1.0, 0.6754966887417219, 0.33879781420765026]\n","val_loss: 0.564 val_accuracy: 0.003 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1398176291793313] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]\n","9-\n","train_loss: 0.594 train_accuracy: 0.002 train_f1: [0.11028315946348732, 0.058823529411764705, 0.2593659942363113, 0.18437225636523263, 0.20044215180545322, 0.08484848484848485, 0.20250284414106937, 0.0, 0.0, 0.0, 0.10230179028132991, 0.12649040953862103] train_fpr: [0.13039901275195392, 0.25672645739910316, 0.24398625429553264, 0.2621609987085665, 0.12792689891490577, 0.2523864070255823, 0.14243759177679882, 0.0, 0.0, 0.0, 0.3704274162495187, 0.6331384015594542] train_fnr: [0.8832807570977917, 0.6805555555555556, 0.7468354430379747, 0.7529411764705882, 0.8635907723169508, 0.7286821705426356, 0.8715728715728716, 1.0, 1.0, 1.0, 0.6026490066225165, 0.3333333333333333]\n","val_loss: 0.555 val_accuracy: 0.003 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1398176291793313] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]\n","10-\n","train_loss: 0.59 train_accuracy: 0.005 train_f1: [0.09119496855345911, 0.03617571059431524, 0.2227204783258595, 0.18761061946902655, 0.1874062968515742, 0.08264462809917356, 0.24136039495337355, 0.0, 0.0, 0.0, 0.09002647837599295, 0.12344398340248963] train_fpr: [0.11929247223364871, 0.2571001494768311, 0.2346588119783996, 0.2578562204046492, 0.12107367218732153, 0.2607865597556319, 0.1593245227606461, 0.0, 0.0, 0.0, 0.3584905660377358, 0.6339181286549708] train_fnr: [0.9085173501577287, 0.8055555555555556, 0.790436005625879, 0.7505882352941177, 0.8746238716148446, 0.7286821705426356, 0.8412698412698413, 1.0, 1.0, 1.0, 0.6622516556291391, 0.34972677595628415]\n","val_loss: 0.551 val_accuracy: 0.003 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1398176291793313] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OYsoiZReFtof"},"source":["**Inference - Model1**"]},{"cell_type":"code","metadata":{"id":"sNo5oolUF4pO","executionInfo":{"status":"ok","timestamp":1611570628297,"user_tz":-60,"elapsed":643,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}}},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TVGYF-DvF0e-","executionInfo":{"status":"ok","timestamp":1611570809803,"user_tz":-60,"elapsed":55713,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"6d8e2f08-d5fb-4226-85d4-16201c14a25a"},"source":["from torch.utils.data import Dataset,DataLoader\r\n","tokenizer_class = transformers.BertTokenizer\r\n","pretrained_weights='distilbert-base-uncased'\r\n","\r\n","class GenreTestDataset(Dataset):\r\n","  \"\"\"Dataset class with pretrained-embeddings from the distil-bert model\r\n","     ARGS: \r\n","          description: total list of text description \r\n","          title: total list of titles\r\n","          labels: one-hot encoded labels \r\n","     Attributes:\r\n","          tokenizer: Tokenizing the text and embedding with ids.\r\n","          max_seq: maximum number of word to consider and truncate \r\n","                   if there are more or pad with [0] they are less.\r\n","          \r\n","     Abreviations:\r\n","     N: Batch size\r\n","     Ci: input Channel size, it is 1 here \r\n","     W: words size\r\n","     D: embedding size\r\n","     Co: output channel  \r\n","     Ks: Kernel size\r\n","     C: classes\r\n","  \"\"\"\r\n","  def __init__(self,description,title):\r\n","    self.title = title\r\n","    self.description = description\r\n","    self.max_seq = 250\r\n","    self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\r\n","  \r\n","  def __len__(self):\r\n","    return len(self.title)\r\n","  \r\n","  def __getitem__(self,idx): \r\n","    # convert each text input to  string without gaps and join they before tokenizing\r\n","    title = \"\".join(self.title[idx].split(\" \"))\r\n","    description = \"\".join(self.description[idx].split(\" \")) \r\n","    inputs = self.tokenizer(title + description, add_special_tokens=True,truncation=True,max_length=self.max_seq)\r\n","    # here input_ids means the token numbers given by the embeddings\r\n","    input_ids = inputs[\"input_ids\"]\r\n","    # token_type_ids ususally helpful if we are using seperate two text data rather than as single text data \r\n","    token_type_ids = inputs[\"token_type_ids\"]\r\n","    # attention_mask will have 1:attending word and 0:padded word\r\n","    attention_mask = inputs[\"attention_mask\"]\r\n","    # here padding with [0] if the tokens are less than the max_seq\r\n","    input_ids = input_ids + [0] * (self.max_seq - len(input_ids))\r\n","    token_type_ids = token_type_ids + [0] * (self.max_seq - len(token_type_ids))\r\n","    attention_mask = attention_mask + [0] * (self.max_seq - len(attention_mask))\r\n","    return {\r\n","        \"input_ids\": torch.tensor(input_ids,dtype=torch.long),\r\n","        \"token_type_ids\": torch.tensor(token_type_ids,dtype=torch.long),\r\n","        \"attention_mask\": torch.tensor(attention_mask,dtype=torch.long)\r\n","    }\r\n","#test_df = pd.read_csv(\"dataset/test.csv\") \r\n","test_dataset = GenreTestDataset(test_df[\"description\"].values,test_df[\"title\"].values)\r\n","test_loader = DataLoader(test_dataset,batch_size=32,shuffle=False,num_workers=4)\r\n","#Instantiating the cnn-bert model with arguments \r\n","embed_dim = 768\r\n","class_num = len(mlb.classes_)\r\n","kernel_num = 3\r\n","kernel_sizes = [2, 3, 4]\r\n","dropout = 0.5\r\n","\r\n","#after running this cell model will be downloaded from web (ignore errors)\r\n","model = simpleCNN(\r\n","    embed_dim=embed_dim,\r\n","    class_num=class_num,\r\n","    kernel_num=kernel_num,\r\n","    kernel_sizes=kernel_sizes,\r\n","    dropout=dropout)\r\n","model.load_state_dict(torch.load(\"model_bert.pth\", map_location=device))\r\n","model.to(device) \r\n","model.eval()\r\n","test_results = test_fn(test_loader,model,device)\r\n","\r\n","# this function will take list of sigmoid values as input, round it to 1/0\r\n","# gets the indices of value 1 or get the index of maximum sigmoid value and return corresponding labels.\r\n","def get_labels(result_labels):\r\n","  result_indices = np.where(np.round(result_labels)==1.0)[1]\r\n","  if len(result_indices)==0:\r\n","    genre = mlb.classes_[np.argmax(result_labels)]\r\n","    return genre\r\n","  else:\r\n","    genres = mlb.classes_[np.where(np.round(result_labels)==1.0)[1]]\r\n","    return genres\r\n","\r\n","test_df[\"genres\"] = test_results\r\n","test_df[\"genres\"] = test_df[\"genres\"].apply(lambda x: get_labels([x]))\r\n","test_df.to_csv(\"test_result_cnn.csv\",index=False)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertModel: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZXL8N4rLDsa5","executionInfo":{"status":"ok","timestamp":1611570228601,"user_tz":-60,"elapsed":10391,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"54f45a72-2563-411c-b222-fd3669ecfec6"},"source":["model = simpleCNN(\n","    embed_dim=embed_dim,\n","    class_num=class_num,\n","    kernel_num=kernel_num,\n","    kernel_sizes=kernel_sizes,\n","    dropout=dropout)\n","#device = \"cpu\"\n","model.load_state_dict(torch.load(\"model_bert.pth\",map_location=device))\n","model.to(device) \n"],"execution_count":62,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertModel: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Av6HzdF-FaIc","executionInfo":{"status":"ok","timestamp":1611570459498,"user_tz":-60,"elapsed":963,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"977a4e04-6d53-4504-a52b-f6b9f898f33b"},"source":["device"],"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"markdown","metadata":{"id":"J1o-ka7fH74K"},"source":["**Prediction - Model1**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vx1ecU0NIA9U","executionInfo":{"status":"ok","timestamp":1611561806000,"user_tz":-60,"elapsed":942,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"84bcf01f-4797-426b-b450-1003b7da0ba0"},"source":["description = \"Set nearly a decade after the finale of the original series, this revival follows Lorelai, Rory and Emily Gilmore through four seasons of change.\"\r\n","title = \"Gilmore Girls: A Year in the Life\"\r\n","test = GenreTestDataset([description,],[title,])\r\n","test_loader = DataLoader(test,batch_size=1,shuffle=False)\r\n","result_labels = test_fn(test_loader,model,device) \r\n","#showing the result which has higest confidence in genres\r\n","result_indices = np.where(np.round(result_labels)==1.0)[1]\r\n","if len(result_indices)==0:\r\n","  print(mlb.classes_[np.argmax(result_labels)])\r\n","else:\r\n","  print(mlb.classes_[np.where(np.round(result_labels)==1.0)[1]])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Thriller']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"__AJITE3PZwq"},"source":["### Model 2: LSTM Model with BERT pre-trained tokenizers"]},{"cell_type":"markdown","metadata":{"id":"_K9vCPZSPjRn"},"source":["**Define the Architecture**"]},{"cell_type":"code","metadata":{"id":"M4CuB9a-Pfgr","executionInfo":{"status":"ok","timestamp":1611571038168,"user_tz":-60,"elapsed":965,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}}},"source":["class GenreLSTM(nn.Module):\r\n","    \r\n","    # define all the layers used in model\r\n","    def __init__(self,embedding_dim, hidden_dim, classes, n_layers=2, \r\n","                 bidirectional=True, dropout=0.2):\r\n","        \r\n","        #Constructor\r\n","        super().__init__()          \r\n","        \r\n","        self.bert_model = transformers.BertModel.from_pretrained(pretrained_weights)\r\n","        #lstm layer\r\n","        self.lstm = nn.LSTM(embedding_dim, \r\n","                           hidden_dim, \r\n","                           num_layers=n_layers, \r\n","                           bidirectional=bidirectional, \r\n","                           dropout=dropout,\r\n","                           batch_first=True)\r\n","        \r\n","        #dense layer \r\n","        self.fc = nn.Linear(hidden_dim * 2, classes)\r\n","\r\n","        \r\n","    def forward(self, ids,att,token):\r\n","        \r\n","        # text = [batch size,sent_length]\r\n","        embedded = self.bert_model(ids,att,token)[0]\r\n","        # embedded = [batch size, sent_len, emb dim]\r\n","\r\n","        packed_output, (hidden, cell) = self.lstm(embedded)\r\n","        #hidden = [batch size, num layers * num directions,hid dim]\r\n","        #cell = [batch size, num layers * num directions,hid dim]\r\n","        \r\n","        #concat the final forward and backward hidden state\r\n","        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\r\n","                \r\n","        #hidden = [batch size, hid dim * num directions]\r\n","        outputs=self.fc(hidden)\r\n","        \r\n","        return outputs"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IjIeXFT_QM4H"},"source":["**Train Model**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eKmeDNOTQNS5","executionInfo":{"status":"ok","timestamp":1611564039025,"user_tz":-60,"elapsed":2168465,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"06a3d1d1-7985-42ef-f3b8-73ce78b1e8ae"},"source":["epochs = 15\r\n","from sklearn.model_selection import train_test_split \r\n","from eval_metric import evaluate_results\r\n","from sklearn.metrics import accuracy_score\r\n","\r\n","train,val = train_test_split(train_df,test_size=0.1)\r\n","\r\n","t_dataset = GenreDataset(train[\"description\"].values,train[\"title\"].values,train[mlb.classes_].values)\r\n","t_loader = DataLoader(t_dataset,batch_size=32,shuffle=True,num_workers=4)\r\n","\r\n","v_dataset = GenreDataset(val[\"description\"].values,val[\"title\"].values,val[mlb.classes_].values)\r\n","v_loader = DataLoader(v_dataset,batch_size=32,shuffle=False,num_workers=4)\r\n","\r\n","model = GenreLSTM(768,100,len(mlb.classes_))\r\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n","optimizer = torch.optim.Adam(model.parameters(),lr=1e-5)\r\n","best_loss = np.inf \r\n","model.to(device) \r\n","print(\"TRAINING STARTED...\")\r\n","\r\n","for e in range(epochs):\r\n","  t_out,t_target,train_loss = train_fn(t_loader,model,optimizer,device)\r\n","  v_out,v_target,val_loss = validation_fn(v_loader,model,device)  \r\n","  acc,f1,fpr,fnr = evaluate_results(np.array(t_target),np.round(t_out))\r\n","  acc2,f1_2,fpr2,fnr2 = evaluate_results(np.array(v_target),np.round(v_out))\r\n","  print(f\"{e+1}-\")\r\n","  #save the model as .pth if the loss is less than the earlier epochs \r\n","  if val_loss < best_loss:\r\n","    torch.save(model.state_dict(),\"model_lstm_bert.pth\")\r\n","    best_loss = val_loss\r\n","  #print((accuracy_score(np.array(t_target),np.round(t_out)),accuracy_score(np.array(v_target),np.round(v_out))))\r\n","  print(\"train_loss:\", round(train_loss,3),\"train_accuracy:\",round(acc,3), \"train_f1:\", f1, \"train_fpr:\", fpr, \"train_fnr:\", fnr) \r\n","  print(\"val_loss:\",round(val_loss,3),\"val_accuracy:\",round(acc2,3), \"val_f1:\", f1_2, \"val_fpr:\", fpr2, \"val_fnr:\", fnr2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertModel: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["TRAINING STARTED...\n","1-\n","train_loss: 0.556 train_accuracy: 0.026 train_f1: [0.04700854700854701, 0.0, 0.10032715376226828, 0.0683111954459203, 0.0019821605550049554, 0.03636363636363636, 0.6567926455566904, 0.011049723756906075, 0.037234042553191495, 0.0, 0.011049723756906077, 0.015325670498084292] train_fpr: [0.05720164609053498, 0.003727171077152441, 0.08170254403131115, 0.03905579399141631, 0.003436426116838488, 0.010711553175210406, 0.912267657992565, 0.009128630705394191, 0.025399426464563703, 0.0, 0.011166730843280709, 0.02887241513850956] train_fnr: [0.9654088050314465, 1.0, 0.9346590909090909, 0.9569377990430622, 0.999001996007984, 0.9776119402985075, 0.08339272986457591, 0.9940828402366864, 0.9771986970684039, 1.0, 0.9933774834437086, 0.9891891891891892]\n","val_loss: 0.496 val_accuracy: 0.02 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6695652173913044, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","2-\n","train_loss: 0.477 train_accuracy: 0.028 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6739393939393938, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9903345724907063, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.009265858873841768, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.451 val_accuracy: 0.02 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6695652173913044, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","3-\n","train_loss: 0.437 train_accuracy: 0.028 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6722154222766218, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9732342007434944, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.021382751247327157, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.42 val_accuracy: 0.02 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6695652173913044, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","4-\n","train_loss: 0.41 train_accuracy: 0.025 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.631311706629055, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7605947955390334, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.20242337847469707, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.401 val_accuracy: 0.02 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6695652173913044, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","5-\n","train_loss: 0.393 train_accuracy: 0.024 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6277732128184058, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8193308550185874, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.18317890235210263, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.389 val_accuracy: 0.02 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6695652173913044, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","6-\n","train_loss: 0.383 train_accuracy: 0.023 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6218057921635434, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7613382899628253, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2195295794725588, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.382 val_accuracy: 0.02 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6695652173913044, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","7-\n","train_loss: 0.376 train_accuracy: 0.023 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6343467543138865, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8104089219330854, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.17462580185317178, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.378 val_accuracy: 0.016 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6033519553072626, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.631578947368421, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2987012987012987, 1.0, 1.0, 1.0, 1.0, 1.0]\n","8-\n","train_loss: 0.372 train_accuracy: 0.02 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5921052631578948, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7070631970260223, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.29436920883820383, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.376 val_accuracy: 0.02 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6695652173913044, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","9-\n","train_loss: 0.37 train_accuracy: 0.024 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6434129326536088, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8364312267657993, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14540270848182466, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.374 val_accuracy: 0.01 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.46037735849056605, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32894736842105265, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6038961038961039, 1.0, 1.0, 1.0, 1.0, 1.0]\n","10-\n","train_loss: 0.368 train_accuracy: 0.02 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5828712261244609, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6669144981412639, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3257305773342837, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.373 val_accuracy: 0.02 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666667, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.006493506493506494, 1.0, 1.0, 1.0, 1.0, 1.0]\n","11-\n","train_loss: 0.367 train_accuracy: 0.019 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5899324739103745, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6646840148698885, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3150392017106201, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.372 val_accuracy: 0.02 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6759259259259259, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.868421052631579, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05194805194805195, 1.0, 1.0, 1.0, 1.0, 1.0]\n","12-\n","train_loss: 0.366 train_accuracy: 0.023 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6380201251019854, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8185873605947955, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16393442622950818, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.372 val_accuracy: 0.007 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27751196172248804, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17105263157894737, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8116883116883117, 1.0, 1.0, 1.0, 1.0, 1.0]\n","13-\n","train_loss: 0.365 train_accuracy: 0.015 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5548738922972051, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.533085501858736, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.41981468282252316, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.371 val_accuracy: 0.01 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4129554655870445, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27631578947368424, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6688311688311688, 1.0, 1.0, 1.0, 1.0, 1.0]\n","14-\n","train_loss: 0.364 train_accuracy: 0.016 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.569620253164557, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5531598513011152, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.39059158945117606, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.37 val_accuracy: 0.02 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.607242339832869, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.631578947368421, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2922077922077922, 1.0, 1.0, 1.0, 1.0, 1.0]\n","15-\n","train_loss: 0.36 train_accuracy: 0.019 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6195516811955168, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6052044609665428, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2908054169636493, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.372 val_accuracy: 0.007 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24875621890547264, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14473684210526316, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8376623376623377, 1.0, 1.0, 1.0, 1.0, 1.0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oHUY4t84Q91z"},"source":["**Inference - Model2**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ATgAu9wQ_dp","executionInfo":{"status":"ok","timestamp":1611571160683,"user_tz":-60,"elapsed":56883,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"46f10ff2-1692-404f-c356-71df29d2e207"},"source":["from torch.utils.data import Dataset,DataLoader\r\n","tokenizer_class = transformers.BertTokenizer\r\n","pretrained_weights='distilbert-base-uncased'\r\n","class LSTMTestDataset(Dataset):\r\n","  \"\"\"Test Dataset class with pretrained-embeddings from the distil-bert model.\r\n","     ARGS: \r\n","          description: total list of text description \r\n","          title: total list of titles\r\n","          labels: one-hot encoded labels \r\n","     Attributes:\r\n","          tokenizer: Tokenizing the text and embedding with ids.\r\n","          max_seq: maximum number of word to consider and truncate \r\n","                   if there are more or pad with [0] they are less.\r\n","          \r\n","     Abreviations:\r\n","     N: Batch size\r\n","     Ci: input Channel size, it is 1 here \r\n","     W: words size\r\n","     D: embedding size\r\n","     Co: output channel  \r\n","     Ks: Kernel size\r\n","     C: classes\r\n","  \"\"\"\r\n","  def __init__(self,description,title):\r\n","    self.title = title\r\n","    self.description = description\r\n","    self.max_seq = 250\r\n","    self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\r\n","  \r\n","  def __len__(self):\r\n","    return len(self.title)\r\n","  \r\n","  def __getitem__(self,idx): \r\n","    #convert each text input to  string without gaps and join they before tokenizing. \r\n","    title = \"\".join(self.title[idx].split(\" \"))\r\n","    description = \"\".join(self.description[idx].split(\" \")) \r\n","    inputs = self.tokenizer(title + description, add_special_tokens=True,truncation=True,max_length=self.max_seq)\r\n","    # here input_ids means the token numbers given by the embeddings\r\n","    input_ids = inputs[\"input_ids\"]\r\n","    # token_type_ids ususally helpful if we are using seperate two text data rather than as single text data \r\n","    token_type_ids = inputs[\"token_type_ids\"]\r\n","    # attention_mask will have 1:attending word and 0:padded word\r\n","    attention_mask = inputs[\"attention_mask\"]\r\n","    #here padding with [0] if the tokens are less than the max_seq\r\n","    input_ids = input_ids + [0] * (self.max_seq - len(input_ids))\r\n","    token_type_ids = token_type_ids + [0] * (self.max_seq - len(token_type_ids))\r\n","    attention_mask = attention_mask + [0] * (self.max_seq - len(attention_mask))\r\n","    return {\r\n","        \"input_ids\": torch.tensor(input_ids,dtype=torch.long),\r\n","        \"token_type_ids\": torch.tensor(token_type_ids,dtype=torch.long),\r\n","        \"attention_mask\": torch.tensor(attention_mask,dtype=torch.long)\r\n","    }\r\n","#test = pd.read_csv(\"dataset/test.csv\") \r\n","test_dataset = LSTMTestDataset(test_df[\"description\"].values,test_df[\"title\"].values)\r\n","test_loader = DataLoader(test_dataset,batch_size=32,shuffle=False,num_workers=4)\r\n","#Instantiating the cnn-bert model with arguments \r\n","#instantiate the model\r\n","model = GenreLSTM(768,100,len(mlb.classes_))\r\n","# load the model to instantiated model\r\n","model.load_state_dict(torch.load(\"model_lstm_bert.pth\"))\r\n","model.to(device) \r\n","test_results = test_fn(test_loader,model,device)\r\n","\r\n","# this function will take list of sigmoid values as input, round it to 1/0\r\n","# gets the indices of value 1 or get the index of maximum sigmoid value and return corresponding labels.\r\n","def get_labels(result_labels):\r\n","  result_indices = np.where(np.round(result_labels)==1.0)[1]\r\n","  if len(result_indices)==0:\r\n","    genre = mlb.classes_[np.argmax(result_labels)]\r\n","    return genre\r\n","  else:\r\n","    genres = mlb.classes_[np.where(np.round(result_labels)==1.0)[1]]\r\n","    return genres\r\n","\r\n","test_df[\"genres\"] = test_results\r\n","test_df[\"genres\"] = test_df[\"genres\"].apply(lambda x: get_labels([x]))\r\n","test_df.to_csv(\"test_result_lstm.csv\",index=False)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertModel: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"rCnZmIcdRsf7"},"source":["**Prediction - Model2**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MmnJYp5CRtAi","executionInfo":{"status":"ok","timestamp":1611564047655,"user_tz":-60,"elapsed":2168165,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"9dc0a67c-4722-4b31-a98c-c3f18bbb8f6e"},"source":["# get the description and title as string  \r\n","description = \"After contracting HIV from a tainted blood treatment, teenaged hemophiliac Ryan White is forced to fight for his right to attend public school.\"\r\n","title = \"The Ryan White Story\"\r\n","test = GenreTestDataset([description,],[title,])\r\n","test_loader = DataLoader(test,batch_size=1,shuffle=False)\r\n","result_labels = test_fn(test_loader,model,device)\r\n","# show the results from the labels-names, which are rounded to 1\r\n","result_indices = np.where(np.round(result_labels)==1.0)[1]\r\n","if len(result_indices)==0:\r\n","  print(mlb.classes_[np.argmax(result_labels)])\r\n","else:\r\n","  print(mlb.classes_[np.where(np.round(result_labels)==1.0)[1]])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['International']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8ub8JKNqR_73"},"source":["### Model 3: GPT-2 model with LSTM using GPT-2 pre-trained tokenizers"]},{"cell_type":"markdown","metadata":{"id":"RQH1jotXSNq8"},"source":["**Dataset Module**"]},{"cell_type":"code","metadata":{"id":"hK0GaDYLSR-U","executionInfo":{"status":"ok","timestamp":1611571306489,"user_tz":-60,"elapsed":799,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}}},"source":["from transformers import GPT2Tokenizer\r\n","\r\n","class GPTDataset(Dataset):\r\n","  \"\"\" This is dataset module for gpt and \r\n","      everything is similar to the earliar dataset modules seen\r\n","      Except the tokenizer and token_ids are not used \r\n","      Tokenizer used here is GPT2  \r\n","  \"\"\"\r\n","  def __init__(self,description,title,labels):\r\n","    self.title = title\r\n","    self.description = description\r\n","    self.labels = labels \r\n","    self.max_seq = 250\r\n","    self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\r\n","  \r\n","  def __len__(self):\r\n","    return len(self.title)\r\n","  \r\n","  def __getitem__(self,idx): \r\n","    title = \"\".join(self.title[idx].split())\r\n","    description = \"\".join(self.description[idx].split()) \r\n","    labels = self.labels[idx,:]\r\n","    inputs = self.tokenizer(title+description, add_special_tokens=True,truncation=True,max_length=self.max_seq)\r\n","    input_ids = inputs[\"input_ids\"]\r\n","    token_type_ids = [0,]\r\n","    attention_mask = inputs[\"attention_mask\"]\r\n","    input_ids = input_ids + [0] * (self.max_seq - len(input_ids))\r\n","    token_type_ids = token_type_ids + [0] * (self.max_seq - len(token_type_ids))\r\n","    attention_mask = attention_mask + [0] * (self.max_seq - len(attention_mask))\r\n","    return {\r\n","        \"input_ids\": torch.tensor(input_ids,dtype=torch.long),\r\n","        \"token_type_ids\": torch.tensor(token_type_ids,dtype=torch.long),\r\n","        \"attention_mask\": torch.tensor(attention_mask,dtype=torch.long),\r\n","        \"labels\": torch.tensor(labels,dtype=torch.float)\r\n","    } "],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zjQtXq5ybsX5"},"source":["**Define the Architecture**"]},{"cell_type":"code","metadata":{"id":"OhkUPCePbvxM","executionInfo":{"status":"ok","timestamp":1611571309689,"user_tz":-60,"elapsed":582,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}}},"source":["from transformers import GPT2Tokenizer, GPT2Model\r\n","class GPTModel(nn.Module):\r\n","    \"\"\"Module with GPT2 model and pretrained from transformers-huggingface.\r\n","     ARGS: \r\n","          embedding_dim: embedding dimention is 768 even for gpt models\r\n","          class_num: ouput label classes \r\n","          hidden_dim: hidden dimentions for lstm models\r\n","    \"\"\"\r\n","    # define all the layers used in model\r\n","    def __init__(self,embedding_dim,hidden_dim, classes):\r\n","        \r\n","        # Constructor\r\n","        super().__init__()          \r\n","        # pre-trained gpt2 model, transfer learning \r\n","        self.model = GPT2Model.from_pretrained('gpt2')  \r\n","        # lstm layer with 2-layers \r\n","        self.lstm = nn.LSTM(embedding_dim, \r\n","                           hidden_dim, \r\n","                           num_layers=2, \r\n","                           bidirectional=True, \r\n","                           dropout=0.2,\r\n","                           batch_first=True)     \r\n","        # dense layer\r\n","        self.fc = nn.Linear(hidden_dim*2, classes)\r\n","\r\n","        \r\n","    def forward(self, in_ids,token,att_mask):\r\n","        \r\n","        #text = [batch size,sent_length]\r\n","        embedded = self.model(input_ids=in_ids,attention_mask=att_mask)[0]              \r\n","        packed_output, (hidden, cell) = self.lstm(embedded)\r\n","        #hidden = [batch size, num layers * num directions,hid dim]\r\n","        #cell = [batch size, num layers * num directions,hid dim]\r\n","        \r\n","        #concat the final forward and backward hidden state\r\n","        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1) \r\n","                \r\n","        #hidden = [batch size, hid dim * num directions]\r\n","        outputs=self.fc(hidden)\r\n","        return outputs "],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"unYAZxMtcJne"},"source":["**Train Model**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3a5e2fdd3ce245cd93349ac8e2bb39b9","463b1ad1d52c47f0a2bda2b70c628f1c","581ddcdbc595404991ce31c58c4ebb78","ab1aeb4531a540e289942bd91234d2f1","2cd1756117564852a0de46e6ec4851f8","a669b6b572b34fb4bc7c675885801695","b502d0da1f574f79b32f60180f7cdae0","905d303dd4af4d86bed4cd9c499dea16","a698716bbdff40d1a1908f217883ccef","7a3302f6e1854b249392afffa20242dd","5b68d8cb6b8347dd9dde0c9d005e32c1","84793bc245744b7f8f8e168c751b7a23","5afad4ddc802489fbafaa0afbd90f575","e963c514febd436a922acb4d1abfaa6a","6da69fbff324478f962721096e3d966d","c369a4266233412c858d05f19604a558","eaef204b16dc48bfa380c3474359d830","c8fc0befc76c493f875f5da2f1a9515e","138ac0c23dc5436fa5650dd0c3265720","c5813404da134772aad223bbaa7fb0ce","8a0ebf1ad7a84da5ad1528ccc624c919","8f7db27016394c5cb37863273d060e0f","4b0dad9231694761b563fd220a0af90e","19fba4b8799b4ef0b2b9a7aeb91a8830","04c7aa6daf844e9fbc7f9836d65daf65","9041e0ae33434b34b0ca3a61b7f03ffd","7aa2ab62ffd54b8abcfe61f561bb4ee0","af8f300bfce7463c9aab5f6234f29988","714f6b9689ac430286fc1fa44034d8d7","118e48a1bda9406daea3496c50f39ff5","824e27c143c14997b971abab9e8f34f1","a9089787b0f24dd4bba183b71f0fc4f3"]},"id":"RFU2Js-Gbv0i","executionInfo":{"status":"ok","timestamp":1611566635310,"user_tz":-60,"elapsed":2407798,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"099d686f-e176-492d-bdb0-1e38334ba5b4"},"source":["epochs = 15\r\n","from sklearn.model_selection import train_test_split\r\n","from eval_metric import evaluate_results\r\n","from sklearn.metrics import accuracy_score\r\n","\r\n","# splitting the train and test dataset \r\n","train,val = train_test_split(train_df,test_size=0.1)\r\n","\r\n","# instantiating dataset class with supplied data and labels \r\n","t_dataset = GPTDataset(train[\"description\"].values,train[\"title\"].values,train[mlb.classes_].values)\r\n","# dataset and batchwise loader from pytorch \r\n","t_loader = DataLoader(t_dataset,batch_size=16,shuffle=True,num_workers=4)\r\n","\r\n","v_dataset = GPTDataset(val[\"description\"].values,val[\"title\"].values,val[mlb.classes_].values)\r\n","v_loader = DataLoader(v_dataset,batch_size=16,shuffle=False,num_workers=4)\r\n","\r\n","model = GPTModel(768,100,len(mlb.classes_))\r\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n","#optimizer methodoly used with very low learning rate \r\n","optimizer = torch.optim.Adam(model.parameters(),lr=1e-5)\r\n","# loadin the model to device for getting the weights to device \r\n","model.to(device) \r\n","best_loss = np.inf\r\n","print(\"TRAINING STARTED...\")\r\n","\r\n","for e in range(epochs):\r\n","  t_out,t_target,train_loss = train_fn(t_loader,model,optimizer,device)\r\n","  v_out,v_target,val_loss = validation_fn(v_loader,model,device)  \r\n","  # here evaluation results from the given file \r\n","  acc,f1,fpr,fnr = evaluate_results(np.array(t_target),np.round(t_out))\r\n","  acc2,f1_2,fpr2,fnr2 = evaluate_results(np.array(v_target),np.round(v_out))\r\n","  print(f\"{e+1}-\")\r\n","  if val_loss < best_loss:\r\n","    torch.save(model.state_dict(),\"model_gpt.pth\")\r\n","    best_loss = val_loss\r\n","  #uncomment below code to get the accuracy score directly from sklearn \r\n","  #print((accuracy_score(np.array(t_target),np.round(t_out)),accuracy_score(np.array(v_target),np.round(v_out))))\r\n","  print(\"train_loss:\", round(train_loss,3),\"train_accuracy:\",round(acc,3), \"train_f1:\", f1, \"train_fpr:\", fpr, \"train_fnr:\", fnr) \r\n","  print(\"val_loss:\",round(val_loss,3),\"val_accuracy:\",round(acc2,3), \"val_f1:\", f1_2, \"val_fpr:\", fpr2, \"val_fnr:\", fnr2)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a5e2fdd3ce245cd93349ac8e2bb39b9","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a698716bbdff40d1a1908f217883ccef","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eaef204b16dc48bfa380c3474359d830","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=665.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"04c7aa6daf844e9fbc7f9836d65daf65","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=548118077.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["TRAINING STARTED...\n","1-\n","train_loss: 0.62 train_accuracy: 0.012 train_f1: [0.0060790273556231, 0.03978779840848806, 0.0027322404371584695, 0.004830917874396135, 0.09573542210617929, 0.0, 0.5763779527559056, 0.0, 0.16132858837485173, 0.02586206896551724, 0.013157894736842106, 0.03902439024390244] train_fpr: [0.00288421920065925, 0.25121133060007456, 0.0009905894006934125, 0.0004280821917808219, 0.04723502304147465, 0.0, 0.6347305389221557, 0.00041203131437989287, 0.19271685761047463, 0.040015243902439025, 0.0007695267410542517, 0.005466614603670442] train_fnr: [0.9968847352024922, 0.7692307692307693, 0.9986282578875172, 0.9975728155339806, 0.9456521739130435, 1.0, 0.3519830028328612, 1.0, 0.7763157894736842, 0.9758064516129032, 0.9932885906040269, 0.9786096256684492]\n","val_loss: 0.539 val_accuracy: 0.026 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6267281105990783, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9503105590062112, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06206896551724138, 1.0, 1.0, 1.0, 1.0, 1.0]\n","2-\n","train_loss: 0.482 train_accuracy: 0.021 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6138555977784274, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.717814371257485, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2563739376770538, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.425 val_accuracy: 0.029 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6430155210643015, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","3-\n","train_loss: 0.405 train_accuracy: 0.024 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6543494996150886, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.905688622754491, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09702549575070822, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.386 val_accuracy: 0.023 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6447058823529412, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8881987577639752, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05517241379310345, 1.0, 1.0, 1.0, 1.0, 1.0]\n","4-\n","train_loss: 0.38 train_accuracy: 0.02 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6095457159286946, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7529940119760479, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.24929178470254956, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.375 val_accuracy: 0.029 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6430155210643015, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","5-\n","train_loss: 0.371 train_accuracy: 0.026 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.669365039210726, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9116766467065869, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.06303116147308782, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.369 val_accuracy: 0.029 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6473429951690821, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8385093167701864, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07586206896551724, 1.0, 1.0, 1.0, 1.0, 1.0]\n","6-\n","train_loss: 0.367 train_accuracy: 0.023 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6687565308254964, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8502994011976048, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09348441926345609, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.366 val_accuracy: 0.02 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6702412868632708, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.639751552795031, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.13793103448275862, 1.0, 1.0, 1.0, 1.0, 1.0]\n","7-\n","train_loss: 0.363 train_accuracy: 0.02 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6638452237001209, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5973053892215568, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22237960339943344, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.365 val_accuracy: 0.02 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6594594594594595, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.639751552795031, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.15862068965517243, 1.0, 1.0, 1.0, 1.0, 1.0]\n","8-\n","train_loss: 0.356 train_accuracy: 0.014 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6798128342245989, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4214071856287425, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2797450424929179, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.356 val_accuracy: 0.003 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6270627062706271, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.004830917874396135, 0.0, 0.391304347826087, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3448275862068966, 1.0, 1.0, 1.0, 1.0, 1.0]\n","9-\n","train_loss: 0.349 train_accuracy: 0.032 train_f1: [0.0, 0.0, 0.0, 0.0, 0.16076058772687984, 0.0, 0.6886008640744433, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.029953917050691243, 0.0, 0.4199101796407186, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 0.908102766798419, 1.0, 0.26628895184135976, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.354 val_accuracy: 0.082 val_f1: [0.0, 0.0, 0.0, 0.0, 0.5024630541871921, 0.0, 0.6549707602339181, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.2560386473429952, 0.0, 0.5279503105590062, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 0.48484848484848486, 1.0, 0.22758620689655173, 1.0, 1.0, 1.0, 1.0, 1.0]\n","10-\n","train_loss: 0.344 train_accuracy: 0.08 train_f1: [0.0, 0.0, 0.0, 0.0, 0.48301420630018527, 0.0, 0.7013500164636154, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.12442396313364056, 0.0, 0.41916167664670656, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 0.6136363636363636, 1.0, 0.24575070821529746, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.348 val_accuracy: 0.069 val_f1: [0.0, 0.0, 0.0, 0.0, 0.5422222222222223, 0.0, 0.6430868167202574, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.3140096618357488, 0.0, 0.40993788819875776, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 0.3838383838383838, 1.0, 0.3103448275862069, 1.0, 1.0, 1.0, 1.0, 1.0]\n","11-\n","train_loss: 0.341 train_accuracy: 0.098 train_f1: [0.0, 0.0, 0.0, 0.0, 0.580547112462006, 0.0, 0.7080491132332878, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.22407834101382487, 0.0, 0.36077844311377244, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 0.43379446640316205, 1.0, 0.26487252124645894, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.355 val_accuracy: 0.098 val_f1: [0.0, 0.0, 0.0, 0.0, 0.6131386861313868, 0.0, 0.6514285714285715, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.4396135265700483, 0.0, 0.5652173913043478, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 0.15151515151515152, 1.0, 0.21379310344827587, 1.0, 1.0, 1.0, 1.0, 1.0]\n","12-\n","train_loss: 0.336 train_accuracy: 0.115 train_f1: [0.0, 0.0, 0.0, 0.0, 0.6483516483516484, 0.0, 0.7109905020352782, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.2672811059907834, 0.0, 0.3652694610778443, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 0.30039525691699603, 1.0, 0.2577903682719547, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.347 val_accuracy: 0.085 val_f1: [0.0, 0.0, 0.0, 0.0, 0.6093749999999999, 0.0, 0.6424242424242425, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.38164251207729466, 0.0, 0.4906832298136646, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 0.21212121212121213, 1.0, 0.2689655172413793, 1.0, 1.0, 1.0, 1.0, 1.0]\n","13-\n","train_loss: 0.332 train_accuracy: 0.118 train_f1: [0.0, 0.0, 0.0, 0.0, 0.6779513888888888, 0.0, 0.7216354344122656, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.29435483870967744, 0.0, 0.3473053892215569, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 0.22826086956521738, 1.0, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.345 val_accuracy: 0.092 val_f1: [0.0, 0.0, 0.0, 0.0, 0.6067415730337079, 0.0, 0.658753709198813, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.42028985507246375, 0.0, 0.5031055900621118, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 0.18181818181818182, 1.0, 0.23448275862068965, 1.0, 1.0, 1.0, 1.0, 1.0]\n","14-\n","train_loss: 0.33 train_accuracy: 0.123 train_f1: [0.0, 0.0, 0.0, 0.0, 0.6766467065868264, 0.0, 0.7229797140006651, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.30817972350230416, 0.0, 0.38023952095808383, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 0.2183794466403162, 1.0, 0.2301699716713881, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.34 val_accuracy: 0.078 val_f1: [0.0, 0.0, 0.0, 0.0, 0.5853658536585367, 0.0, 0.6305732484076434, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.36231884057971014, 0.0, 0.43478260869565216, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 0.2727272727272727, 1.0, 0.31724137931034485, 1.0, 1.0, 1.0, 1.0, 1.0]\n","15-\n","train_loss: 0.324 train_accuracy: 0.124 train_f1: [0.0, 0.0, 0.0, 0.0, 0.6943363597060094, 0.0, 0.7384305835010061, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.2868663594470046, 0.0, 0.35104790419161674, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 0.20652173913043478, 1.0, 0.22025495750708216, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.343 val_accuracy: 0.062 val_f1: [0.0, 0.0, 0.0, 0.0, 0.4873096446700508, 0.0, 0.6015037593984963, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.24154589371980675, 0.0, 0.2546583850931677, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 0.5151515151515151, 1.0, 0.4482758620689655, 1.0, 1.0, 1.0, 1.0, 1.0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VCr7IdWGcwQO"},"source":["**Inference - Model3**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0oD2bdiec1Op","executionInfo":{"status":"ok","timestamp":1611571379202,"user_tz":-60,"elapsed":63342,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"83824441-4265-46c2-ed5e-5158fb753db0"},"source":["from transformers import GPT2Tokenizer\r\n","\r\n","class GPTTestDataset(Dataset):\r\n","  \"\"\" This is dataset module for gpt and \r\n","      everything is similar to the earliar dataset modules seen. \r\n","      Except the tokenizer and token_ids are not used \r\n","      Tokenizer used here is GPT2  \r\n","  \"\"\"\r\n","  def __init__(self,description,title):\r\n","    self.title = title\r\n","    self.description = description\r\n","    self.max_seq = 250\r\n","    self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\r\n","  \r\n","  def __len__(self):\r\n","    return len(self.title)\r\n","  \r\n","  def __getitem__(self,idx): \r\n","    title = \"\".join(self.title[idx].split())\r\n","    description = \"\".join(self.description[idx].split()) \r\n","    inputs = self.tokenizer(title+description, add_special_tokens=True,truncation=True,max_length=self.max_seq)\r\n","    input_ids = inputs[\"input_ids\"]\r\n","    token_type_ids = [0,] \r\n","    attention_mask = inputs[\"attention_mask\"]\r\n","    input_ids = input_ids + [0] * (self.max_seq - len(input_ids))\r\n","    token_type_ids = token_type_ids + [0] * (self.max_seq - len(token_type_ids))\r\n","    attention_mask = attention_mask + [0] * (self.max_seq - len(attention_mask))\r\n","    return {\r\n","        \"input_ids\": torch.tensor(input_ids,dtype=torch.long),\r\n","        \"token_type_ids\": torch.tensor(token_type_ids,dtype=torch.long),\r\n","        \"attention_mask\": torch.tensor(attention_mask,dtype=torch.long),\r\n","    } \r\n","#test = pd.read_csv(\"dataset/test.csv\") \r\n","test_dataset = GPTTestDataset(test_df[\"description\"].values,test_df[\"title\"].values)\r\n","test_loader = DataLoader(test_dataset,batch_size=32,shuffle=False,num_workers=4)\r\n","#Instantiating the cnn-bert model with arguments \r\n","model = GPTModel(768,100,len(mlb.classes_))\r\n","model.load_state_dict(torch.load(\"model_gpt.pth\"))\r\n","model.to(device) \r\n","test_results = test_fn(test_loader,model,device) \r\n","\r\n","# this function will take list of sigmoid values as input, round it to 1/0\r\n","# gets the indices of value 1 or get the index of maximum sigmoid value and return corresponding labels.\r\n","def get_labels(result_labels):\r\n","  result_indices = np.where(np.round(result_labels)==1.0)[1]\r\n","  if len(result_indices)==0:\r\n","    genre = mlb.classes_[np.argmax(result_labels)]\r\n","    return genre\r\n","  else:\r\n","    genres = mlb.classes_[np.where(np.round(result_labels)==1.0)[1]]\r\n","    return genres\r\n","\r\n","test_df[\"genres\"] = test_results\r\n","test_df[\"genres\"] = test_df[\"genres\"].apply(lambda x: get_labels([x]))\r\n","test_df.to_csv(\"test_result_gpt.csv\",index=False)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"OYOoJ-KKc1jP"},"source":["**Prediction - Model3**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hAgpCK-qc50j","executionInfo":{"status":"ok","timestamp":1611566660723,"user_tz":-60,"elapsed":2425832,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"064d9009-51f8-41a7-95c4-f9ac4f4b6aea"},"source":["# get the description and title as string  \r\n","description = \"Banished to a wasteland of undesirables, a young woman struggles to find her feet among a drug-soaked desert society and an enclave of cannibals.\"\r\n","title = \"The Bad Batch\"\r\n","test = GPTTestDataset([description,],[title,])\r\n","test_loader = DataLoader(test,batch_size=1,shuffle=False)\r\n","result_labels = test_fn(test_loader,model,device)\r\n","# show the results from the labels-names, which are rounded to 1\r\n","result_indices = np.where(np.round(result_labels)==1.0)[1]\r\n","if len(result_indices)==0:\r\n","  print(mlb.classes_[np.argmax(result_labels)])\r\n","else:\r\n","  print(mlb.classes_[np.where(np.round(result_labels)==1.0)[1]])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Drama' 'International']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_spw66sHfzAt"},"source":["### Conlcusion"]},{"cell_type":"markdown","metadata":{"id":"CN4nrsJtf4AW"},"source":["Overall deep learning models didn't perform well on the given dataset. This could be attributed to the fact that the dataset size was pretty small.\r\n","\r\n","Also, Model 3 based on accuracy performed better compared to other models. This is possibly because GPT-2 tokenizer is pre-trained on vast set of resources while BERT tokenizer is limited to Wikipedia only."]},{"cell_type":"code","metadata":{"id":"imCeudbUgmhY"},"source":[""],"execution_count":null,"outputs":[]}]}