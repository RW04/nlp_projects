{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"movie_genre_classification_DL_v2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOmP5+nKLitf0KFPe19IG9N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"32c7f2bafe3045bea07096d735699ab3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_79490daf508f4abc864d11fe69ebfd47","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_60c546e42a68437c99ac0baa3df54e42","IPY_MODEL_5d5f4f237a9b43baa776313dfc7b7840"]}},"79490daf508f4abc864d11fe69ebfd47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"60c546e42a68437c99ac0baa3df54e42":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7e9da4cba55e402897e1eab21077f8b0","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":442,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":442,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2479cb50056644fd8a786a803420fcb0"}},"5d5f4f237a9b43baa776313dfc7b7840":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_dacae6d96d644e8b83c345f8e1fced3c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 442/442 [00:00&lt;00:00, 1.79kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6ae4e6f5842d45a2a67c8ce44bbb84d7"}},"7e9da4cba55e402897e1eab21077f8b0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2479cb50056644fd8a786a803420fcb0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dacae6d96d644e8b83c345f8e1fced3c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6ae4e6f5842d45a2a67c8ce44bbb84d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"51dda6ec43f54782a50be44c6a9ef8c3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_77a0479aa3f9493390d1134c8861c1a1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f2ce4113241c40c6b3c6aa61ade1e00b","IPY_MODEL_b44a6463fce94c5e8a7bbe45b1442f0f"]}},"77a0479aa3f9493390d1134c8861c1a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f2ce4113241c40c6b3c6aa61ade1e00b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7d4e351fc1b34d4f92b15bd4a925794c","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":267967963,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":267967963,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9ab26ca6a6c947249830ce066c43baa3"}},"b44a6463fce94c5e8a7bbe45b1442f0f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_41f4479fbac9400d9318f70b32836c27","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 268M/268M [00:04&lt;00:00, 66.3MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7e50440299df43b48775319623bfb1e4"}},"7d4e351fc1b34d4f92b15bd4a925794c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9ab26ca6a6c947249830ce066c43baa3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"41f4479fbac9400d9318f70b32836c27":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7e50440299df43b48775319623bfb1e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e13907f86b974269adfa64d9b6c0b652":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_40bf8eea7a48461483ad10a0bdeb03a9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cc55ec9dc0814a39958db6c16144d098","IPY_MODEL_1e2128fc934e495bbe8db71e2ea972b4"]}},"40bf8eea7a48461483ad10a0bdeb03a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cc55ec9dc0814a39958db6c16144d098":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4016ca956a7e48308536add21e0ea90b","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3f5a6b76833649a29251c060ae9d06b0"}},"1e2128fc934e495bbe8db71e2ea972b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cb478a3237b6400897c2ef375a498bec","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:01&lt;00:00, 209kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_73ffb12fb670499e8c8b66d20af458be"}},"4016ca956a7e48308536add21e0ea90b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3f5a6b76833649a29251c060ae9d06b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cb478a3237b6400897c2ef375a498bec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"73ffb12fb670499e8c8b66d20af458be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0740d5aa8cde4364aa615a43df05fe48":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_49a5da75680e42b880bfb7e77cdf1f84","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_26ce0a1615aa4dab8f3724cfed34e1ee","IPY_MODEL_d4b3adc554df4d1bbd1940f81c9963ce"]}},"49a5da75680e42b880bfb7e77cdf1f84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"26ce0a1615aa4dab8f3724cfed34e1ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7ebd858fd0a5469c9ca2160b197935a1","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8895f07fb5fd47ad80f90668fb4ae65a"}},"d4b3adc554df4d1bbd1940f81c9963ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_84c509cbc6de48b08a703f218de670b8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:00&lt;00:00, 35.2B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8a70b54482984839bf653ca7763ba091"}},"7ebd858fd0a5469c9ca2160b197935a1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8895f07fb5fd47ad80f90668fb4ae65a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"84c509cbc6de48b08a703f218de670b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8a70b54482984839bf653ca7763ba091":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6f78101f68ab49e99cd368580c895088":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5fe304905a184667a2954d3acc56d4a4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_19b11bf4ed7c43b0bbbdf582a8725237","IPY_MODEL_6c13410262964d218c4bd634bf164811"]}},"5fe304905a184667a2954d3acc56d4a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"19b11bf4ed7c43b0bbbdf582a8725237":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_85972ac0c208406d888b6bda79adf301","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0aa7e03535f2430ea833ff41e7a2fafc"}},"6c13410262964d218c4bd634bf164811":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1cc6c0c49df949de85f7e785e2cd8928","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 466k/466k [00:00&lt;00:00, 1.36MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_44adc367ed1a472a898fb84609faae8c"}},"85972ac0c208406d888b6bda79adf301":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0aa7e03535f2430ea833ff41e7a2fafc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1cc6c0c49df949de85f7e785e2cd8928":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"44adc367ed1a472a898fb84609faae8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"370971a7ab2540ec802652b374834fc5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8347b927802e484b9f7a280c0b18dc6a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a430364789e2457ab9b673391e4b3242","IPY_MODEL_a0867a3830814088becaa4b49b6046ec"]}},"8347b927802e484b9f7a280c0b18dc6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a430364789e2457ab9b673391e4b3242":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_411f54112eb64038884cfbe4211de785","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1042301,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1042301,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_31e61dfb88494173894755a41ac68ae2"}},"a0867a3830814088becaa4b49b6046ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e340fc19f55246a2b113107daf29dd5f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.04M/1.04M [00:01&lt;00:00, 889kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_185fd7445b644f109ba78cd75805e446"}},"411f54112eb64038884cfbe4211de785":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"31e61dfb88494173894755a41ac68ae2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e340fc19f55246a2b113107daf29dd5f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"185fd7445b644f109ba78cd75805e446":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"19c6b3cdeb674380b4fa56693b4b7ca7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9950ce2dd4e449fca1ce7b8899ea6dbe","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_12cf21241b894c65bc15960a3630a190","IPY_MODEL_91abf79295b343b7b56804c345b1db50"]}},"9950ce2dd4e449fca1ce7b8899ea6dbe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"12cf21241b894c65bc15960a3630a190":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7eb122534940440db7f25ef34853d4c9","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":456318,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":456318,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_191a18f2d68c44f9934a2dcabaa87b66"}},"91abf79295b343b7b56804c345b1db50":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3ed066e3702a4f5eb0626472786b3f01","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 456k/456k [00:00&lt;00:00, 1.27MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ff3eb8a34f384336a7e15968aac270a1"}},"7eb122534940440db7f25ef34853d4c9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"191a18f2d68c44f9934a2dcabaa87b66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3ed066e3702a4f5eb0626472786b3f01":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ff3eb8a34f384336a7e15968aac270a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a0188906e4434ffd9fa59eacc1d1df95":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_26c0c479f0b84991bde56c2ac38858c5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7d15d276ec604c738097a7e2f642c596","IPY_MODEL_a83a9b71e71c454cbc6434eee53d3b2f"]}},"26c0c479f0b84991bde56c2ac38858c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7d15d276ec604c738097a7e2f642c596":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e6880d7665d84a08bee7899c61804f08","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":665,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":665,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_373f84344e9343a59e3bcd704863277f"}},"a83a9b71e71c454cbc6434eee53d3b2f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5656988ad62e470e80413b2aa18db6a8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 665/665 [05:43&lt;00:00, 1.94B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c4f44b2efa39435b8196d517441701f0"}},"e6880d7665d84a08bee7899c61804f08":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"373f84344e9343a59e3bcd704863277f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5656988ad62e470e80413b2aa18db6a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c4f44b2efa39435b8196d517441701f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6332cfa1bc4c4b79864da45dc0e0a08a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1bca82410ab54f6d9527b0c39fe81d62","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a840abb58cf34298bebacdd128f1fc1c","IPY_MODEL_3ea31b45e49341b0b37b0cca09ba04ac"]}},"1bca82410ab54f6d9527b0c39fe81d62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a840abb58cf34298bebacdd128f1fc1c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_570eea3d9bdf4512ad77c2087c129001","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":548118077,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":548118077,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4802d50e116647c19e49389dad1c7786"}},"3ea31b45e49341b0b37b0cca09ba04ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b97bbb06082d49449ccb241fa17c26af","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 548M/548M [00:10&lt;00:00, 50.1MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3de954b0fb804ea09b4a77cf71ad7475"}},"570eea3d9bdf4512ad77c2087c129001":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4802d50e116647c19e49389dad1c7786":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b97bbb06082d49449ccb241fa17c26af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3de954b0fb804ea09b4a77cf71ad7475":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"vyO-oNMNj0KK"},"source":["This notebook demonstrates the implementation of deep learning models to classify genre of a given set of movies."]},{"cell_type":"markdown","metadata":{"id":"OUzfTYw6ImOL"},"source":["Training Dataset: train.csv\n","\n","Test Dataset: test.csv\n","\n","To make life easy, ​word embeddings are already extracted using BERT \n","\n","The goal is to train 3 different deep learning models far as this task is concerned,<br><br>\n","- Model1: Train at least 1 CNN on the BERT embeddings<br>\n","- Model2: Train any model of your choice on the BERT embeddings\n","- Model3: Train any other model of your choice + different embeddings other than BERT\n","\n","and evaluate the predictions on the test dataset\n"]},{"cell_type":"markdown","metadata":{"id":"FyXDcpPKH6YF"},"source":["### Accessing Google Drive from Google Colab"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r2bJfBKOHsD3","executionInfo":{"status":"ok","timestamp":1611742186328,"user_tz":-60,"elapsed":27676,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"66e957a3-09d1-4d20-a781-9dfea49816e7"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LSou2g4PjJop"},"source":["Setting path variables for easier imports in google colab"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5s5Ia5QUm2Jq","executionInfo":{"status":"ok","timestamp":1611742188493,"user_tz":-60,"elapsed":654,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"6081921c-f039-43b4-be11-8d3756b7e27a"},"source":["%cd /content/gdrive/MyDrive/colab_notebooks/genre_classification/\n","import sys\n","sys.path.insert(0,'/content/gdrive/MyDrive/colab_notebooks/genre_classification')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive/colab_notebooks/genre_classification\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ozKDvIQ4IhR2"},"source":["### Install, Import required libraries"]},{"cell_type":"markdown","metadata":{"id":"As1jRzzCJu0c"},"source":["If you are working in google colab, run this cell, else run the following one"]},{"cell_type":"code","metadata":{"id":"VlnWgiVcJkVZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611742195946,"user_tz":-60,"elapsed":7348,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"a8fe4b84-ae0c-4c64-e999-5e8167c78777"},"source":["# Install Transformer library for models and tokenizer\n","\n","!pip install -U -q transformers "],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 1.8MB 8.6MB/s \n","\u001b[K     |████████████████████████████████| 2.9MB 27.4MB/s \n","\u001b[K     |████████████████████████████████| 890kB 51.8MB/s \n","\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rS5nNbm7m-yf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611742211013,"user_tz":-60,"elapsed":585,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"de6e2e70-8461-4910-9ccf-b2df1e5b66db"},"source":["# Importing the required libraries\n","import numpy as np\n","import pandas as pd\n","\n","from tqdm import tqdm\n","\n","import transformers\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader\n","\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","torch.manual_seed(2413)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fe9f02655d0>"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"L6e7UZ7GKGto"},"source":["### Load, Read the Data"]},{"cell_type":"code","metadata":{"id":"iW3RGk0yKLhO"},"source":["# Read data from .csv file\n","train_df = pd.read_csv('data/train.csv')\n","test_df = pd.read_csv('data/test.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jVhE5Up_LTb_","executionInfo":{"status":"ok","timestamp":1611742215830,"user_tz":-60,"elapsed":1300,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"35689c7b-e7de-49e0-d695-40402faf0b68"},"source":["# Print the shape of train and test dataframe(s)\n","print(\"Training dataset has {} rows and {} columns\".format(train_df.shape[0], train_df.shape[1]))\n","print(\"Test dataset has {} rows and {} columns\".format(test_df.shape[0], test_df.shape[1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training dataset has 3054 rows and 8 columns\n","Test dataset has 3054 rows and 7 columns\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":272},"id":"Y5ikHJPDOf1m","executionInfo":{"status":"ok","timestamp":1611742217225,"user_tz":-60,"elapsed":799,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"40d7321e-0a21-466b-c160-b83366e30796"},"source":["# Display first 5 rows of training dataset\n","train_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>type</th>\n","      <th>title</th>\n","      <th>director</th>\n","      <th>cast</th>\n","      <th>country</th>\n","      <th>rating</th>\n","      <th>description</th>\n","      <th>genres</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Movie</td>\n","      <td>The Ryan White Story</td>\n","      <td>John Herzfeld</td>\n","      <td>Judith Light, Lukas Haas, Michael Bowen, Nikki...</td>\n","      <td>United States</td>\n","      <td>TV-PG</td>\n","      <td>After contracting HIV from a tainted blood tre...</td>\n","      <td>Drama</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Movie</td>\n","      <td>Mumbai Cha Raja</td>\n","      <td>Manjeet Singh</td>\n","      <td>Rahul Bairagi, Arbaaz Khan, Tejas Parvatkar, D...</td>\n","      <td>India</td>\n","      <td>TV-14</td>\n","      <td>This coming-of-age tale follows Rahul, a young...</td>\n","      <td>Drama, International</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Movie</td>\n","      <td>Soekarno</td>\n","      <td>Hanung Bramantyo</td>\n","      <td>Ario Bayu, Lukman Sardi, Maudy Koesnaedi, Tant...</td>\n","      <td>Indonesia</td>\n","      <td>TV-MA</td>\n","      <td>This biographical drama about Indonesia's firs...</td>\n","      <td>Drama, International</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Movie</td>\n","      <td>The Young Offenders</td>\n","      <td>Peter Foott</td>\n","      <td>Alex Murphy, Chris Walley, Hilary Rose, Domini...</td>\n","      <td>Ireland</td>\n","      <td>TV-MA</td>\n","      <td>Never ones to think things through, two Irish ...</td>\n","      <td>Comedy, International</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Movie</td>\n","      <td>The King</td>\n","      <td>David Michôd</td>\n","      <td>Timothée Chalamet, Joel Edgerton, Robert Patti...</td>\n","      <td>NaN</td>\n","      <td>R</td>\n","      <td>Wayward Prince Hal must turn from carouser to ...</td>\n","      <td>Drama</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    type  ...                 genres\n","0  Movie  ...                  Drama\n","1  Movie  ...   Drama, International\n","2  Movie  ...   Drama, International\n","3  Movie  ...  Comedy, International\n","4  Movie  ...                  Drama\n","\n","[5 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":289},"id":"bxtHFsv4Ovcw","executionInfo":{"status":"ok","timestamp":1611742218736,"user_tz":-60,"elapsed":501,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"95284483-e275-4cc7-e305-8e146d2d85dc"},"source":["# Display first 5 rows of test dataset\n","test_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>type</th>\n","      <th>title</th>\n","      <th>director</th>\n","      <th>cast</th>\n","      <th>country</th>\n","      <th>rating</th>\n","      <th>description</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Movie</td>\n","      <td>The Bill Murray Stories: Life Lessons Learned ...</td>\n","      <td>Tommy Avallone</td>\n","      <td>Tommy Avallone, Bill Murray, Joel Murray, Pete...</td>\n","      <td>United States</td>\n","      <td>TV-MA</td>\n","      <td>This documentary highlights spontaneous encoun...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Movie</td>\n","      <td>The Short Game</td>\n","      <td>Josh Greenbaum</td>\n","      <td>Sky Sudberry, Allan Kournikova, Jed Dy, Zamoku...</td>\n","      <td>United States</td>\n","      <td>PG</td>\n","      <td>They are fiercely competitive athletes, determ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Movie</td>\n","      <td>The Bad Batch</td>\n","      <td>Ana Lily Amirpour</td>\n","      <td>Suki Waterhouse, Jason Momoa, Keanu Reeves, Ji...</td>\n","      <td>United States</td>\n","      <td>R</td>\n","      <td>Banished to a wasteland of undesirables, a you...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>TV Show</td>\n","      <td>The Twilight Zone (Original Series)</td>\n","      <td>NaN</td>\n","      <td>Rod Serling</td>\n","      <td>United States</td>\n","      <td>TV-PG</td>\n","      <td>Hosted by creator Rod Serling, this groundbrea...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Movie</td>\n","      <td>World Trade Center</td>\n","      <td>Oliver Stone</td>\n","      <td>Nicolas Cage, Michael Peña, Maggie Gyllenhaal,...</td>\n","      <td>United States</td>\n","      <td>PG-13</td>\n","      <td>Working under treacherous conditions, an army ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      type  ...                                        description\n","0    Movie  ...  This documentary highlights spontaneous encoun...\n","1    Movie  ...  They are fiercely competitive athletes, determ...\n","2    Movie  ...  Banished to a wasteland of undesirables, a you...\n","3  TV Show  ...  Hosted by creator Rod Serling, this groundbrea...\n","4    Movie  ...  Working under treacherous conditions, an army ...\n","\n","[5 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x2oFB6PKtmGx","executionInfo":{"status":"ok","timestamp":1611742220718,"user_tz":-60,"elapsed":544,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"36fcdd19-22ce-490d-b451-77381a07f1d2"},"source":["# Features of training dataframe\n","train_df.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 3054 entries, 0 to 3053\n","Data columns (total 8 columns):\n"," #   Column       Non-Null Count  Dtype \n","---  ------       --------------  ----- \n"," 0   type         3054 non-null   object\n"," 1   title        3054 non-null   object\n"," 2   director     2112 non-null   object\n"," 3   cast         2767 non-null   object\n"," 4   country      2838 non-null   object\n"," 5   rating       3053 non-null   object\n"," 6   description  3054 non-null   object\n"," 7   genres       3054 non-null   object\n","dtypes: object(8)\n","memory usage: 191.0+ KB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_RPUwCRXt1cP","executionInfo":{"status":"ok","timestamp":1611742222961,"user_tz":-60,"elapsed":535,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"a5bd3ce8-b5ef-4afa-db60-c7492dd3f2c2"},"source":["# Features of test dataframe\n","test_df.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 3054 entries, 0 to 3053\n","Data columns (total 7 columns):\n"," #   Column       Non-Null Count  Dtype \n","---  ------       --------------  ----- \n"," 0   type         3054 non-null   object\n"," 1   title        3054 non-null   object\n"," 2   director     2106 non-null   object\n"," 3   cast         2791 non-null   object\n"," 4   country      2831 non-null   object\n"," 5   rating       3052 non-null   object\n"," 6   description  3054 non-null   object\n","dtypes: object(7)\n","memory usage: 167.1+ KB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UNTOZ19xKCtV"},"source":["- For predicting genre, we will take into account `title` and `description` features only for obvious reasons and these don't have any missing values records/observations.\n","- Based on the nature of the dataset, this is a multi-label classification task\n","- Features: `title`, `description`\n","- Label: `genres`"]},{"cell_type":"markdown","metadata":{"id":"ztMzS6h4xXvW"},"source":["We will now segregate `genres` for every observations followed by converting them into one-hot encoded matrices"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AsfrHt4Kvulp","executionInfo":{"status":"ok","timestamp":1611742226788,"user_tz":-60,"elapsed":539,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"e426230c-86a5-42d6-8b2d-46bf854edc83"},"source":["# Split individual genres\n","train_df[\"genres\"] = train_df.genres.apply(lambda x: [i.strip() for i in x.split(\",\")])\n","\n","# Initialize multi-label transformer\n","mlb = MultiLabelBinarizer()\n","\n","# Create a new dataframe 'ohe_labels' having one-hot encoded representation\n","ohe_labels = pd.DataFrame(mlb.fit_transform(train_df.genres), columns=mlb.classes_)\n","\n","# Merge 'train_df' and 'ohe_labels'\n","train_df = pd.concat((train_df, ohe_labels), axis=1)\n","\n","# Display the different genres available\n","print(\"These are the labels exising in the dataset\\n\", mlb.classes_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["These are the labels exising in the dataset\n"," ['Action' 'Anime' 'Comedy' 'Documentaries' 'Drama' 'Horror'\n"," 'International' 'Kids' 'Romantic' 'Sci-Fi & Fantasy' 'Stand-up'\n"," 'Thriller']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":369},"id":"sok3b48Xve48","executionInfo":{"status":"ok","timestamp":1611742228415,"user_tz":-60,"elapsed":373,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"18671994-0a95-4ed1-9fec-a46354f1971e"},"source":["# Display first two rows of 'train_df' post transformation\n","train_df.head(2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>type</th>\n","      <th>title</th>\n","      <th>director</th>\n","      <th>cast</th>\n","      <th>country</th>\n","      <th>rating</th>\n","      <th>description</th>\n","      <th>genres</th>\n","      <th>Action</th>\n","      <th>Anime</th>\n","      <th>Comedy</th>\n","      <th>Documentaries</th>\n","      <th>Drama</th>\n","      <th>Horror</th>\n","      <th>International</th>\n","      <th>Kids</th>\n","      <th>Romantic</th>\n","      <th>Sci-Fi &amp; Fantasy</th>\n","      <th>Stand-up</th>\n","      <th>Thriller</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Movie</td>\n","      <td>The Ryan White Story</td>\n","      <td>John Herzfeld</td>\n","      <td>Judith Light, Lukas Haas, Michael Bowen, Nikki...</td>\n","      <td>United States</td>\n","      <td>TV-PG</td>\n","      <td>After contracting HIV from a tainted blood tre...</td>\n","      <td>[Drama]</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Movie</td>\n","      <td>Mumbai Cha Raja</td>\n","      <td>Manjeet Singh</td>\n","      <td>Rahul Bairagi, Arbaaz Khan, Tejas Parvatkar, D...</td>\n","      <td>India</td>\n","      <td>TV-14</td>\n","      <td>This coming-of-age tale follows Rahul, a young...</td>\n","      <td>[Drama, International]</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    type                 title  ... Stand-up Thriller\n","0  Movie  The Ryan White Story  ...        0        0\n","1  Movie       Mumbai Cha Raja  ...        0        0\n","\n","[2 rows x 20 columns]"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"kaXdAHV01yIq"},"source":["### Model 1: CNN Model with BERT pre-trained tokenizers"]},{"cell_type":"markdown","metadata":{"id":"m4nOAMVP2dWN"},"source":["**Define the Architecture**"]},{"cell_type":"code","metadata":{"id":"crWzLS6YS5VB"},"source":["class simpleCNN(nn.Module):\n","    \"\"\"Model that uses CNN and with pretrained-model- distil-bert model\n","     ARGS: \n","          embed_dim: embedding dimension is 768 for BERT models\n","          class_num: total number of labels\n","          kernel_num: Number of kernels in cnn\n","          kernel_size: list of kernels of sizes for different kernel\n","          dropout: dropout to be used for regularization \n","     Attributes:\n","          bert_model: BERT model with pretrianed weights for transfer learning\n","          convs1: list of cnn model with respective kernel size and dimensions\n","          fc: final linear/dense layer with class_num output\n","     Abreviations:\n","     N: Batch size\n","     Ci: input Channel size, it is 1 here \n","     W: words size\n","     D: embedding size\n","     Co: output channel  \n","     Ks: Kernel size\n","     C: classes\n","    \"\"\"\n","    def __init__(self, embed_dim, class_num, kernel_num, kernel_sizes, dropout):\n","        super(simpleCNN, self).__init__()\n","        self.bert_model = transformers.BertModel.from_pretrained(pretrained_weights)\n","        #pytorch uses special-list comprehension dedicated to its model classes\n","        self.convs1 = nn.ModuleList([nn.Conv2d(1, kernel_num, (K, embed_dim)) for K in kernel_sizes])\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc1 = nn.Linear(len(kernel_sizes) * kernel_num, class_num) \n","        \n","    def forward(self, ids,att,token):\n","        x_ = self.bert_model(ids,att,token)[0] \n","        x = x_.unsqueeze(1)  # (N, Ci, W, D) \n","        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, W), ...]*len(Ks) \n","        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)\n","        x = torch.cat(x, 1)\n","        x = self.dropout(x)  # (N, len(Ks)*Co)\n","        logit = self.fc1(x)  # (N, C)\n","        return logit "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"coUOqQNJ4v5v"},"source":["**Dataset Module** "]},{"cell_type":"code","metadata":{"id":"2Y7xpEf3TNtV"},"source":["from torch.utils.data import Dataset,DataLoader\n","tokenizer_class = transformers.BertTokenizer\n","pretrained_weights='distilbert-base-uncased'\n","class GenreDataset(Dataset):\n","  \"\"\"Dataset class with pretrained-embeddings from the distil-bert model.\n","     ARGS: \n","          description: total list of text description \n","          title: total list of titles\n","          labels: one-hot encoded labels \n","     Attributes:\n","          tokenizer: Tokenizing the text and embedding with ids.\n","          max_seq: maximum number of word to consider and truncate \n","                   if there are more or pad with [0] they are less.\n","          \n","     Abreviations:\n","     N: Batch size\n","     Ci: input Channel size, it is 1 here \n","     W: words size\n","     D: embedding size\n","     Co: output channel  \n","     Ks: Kernel size\n","     C: classes\n","  \"\"\"\n","  def __init__(self,description,labels):\n","    self.description = description\n","    self.labels = labels \n","    self.max_seq = 250 #shouldn't be > 512 for BERT\n","    self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n","  \n","  def __len__(self):\n","    return len(self.description)\n","  \n","  def __getitem__(self,idx): \n","    #convert each text input to string without gaps and join them before tokenizing\n","    description = \"\".join(self.description[idx].split()) \n","    labels = self.labels[idx,:]\n","    inputs = self.tokenizer( description, add_special_tokens=True,truncation=True,max_length=self.max_seq)\n","    # here 'input_ids' implies the token numbers given by the embeddings\n","    input_ids = inputs[\"input_ids\"]\n","    # 'token_type_ids' usually helpful if we are using separate two text data rather than single text data \n","    token_type_ids = inputs[\"token_type_ids\"]\n","    # 'attention_mask' will have 1:attending word and 0:padded word\n","    attention_mask = inputs[\"attention_mask\"]\n","    #here padding with [0] if the tokens are less than the max_seq\n","    input_ids = input_ids + [0] * (self.max_seq - len(input_ids))\n","    token_type_ids = token_type_ids + [0] * (self.max_seq - len(token_type_ids))\n","    attention_mask = attention_mask + [0] * (self.max_seq - len(attention_mask))\n","    return {\n","        \"input_ids\": torch.tensor(input_ids,dtype=torch.long),\n","        \"token_type_ids\": torch.tensor(token_type_ids,dtype=torch.long),\n","        \"attention_mask\": torch.tensor(attention_mask,dtype=torch.long),\n","        \"labels\": torch.tensor(labels,dtype=torch.float)\n","    } \n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6_cpTnmI5T9P"},"source":["**Defining Hyperparameters**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220,"referenced_widgets":["32c7f2bafe3045bea07096d735699ab3","79490daf508f4abc864d11fe69ebfd47","60c546e42a68437c99ac0baa3df54e42","5d5f4f237a9b43baa776313dfc7b7840","7e9da4cba55e402897e1eab21077f8b0","2479cb50056644fd8a786a803420fcb0","dacae6d96d644e8b83c345f8e1fced3c","6ae4e6f5842d45a2a67c8ce44bbb84d7","51dda6ec43f54782a50be44c6a9ef8c3","77a0479aa3f9493390d1134c8861c1a1","f2ce4113241c40c6b3c6aa61ade1e00b","b44a6463fce94c5e8a7bbe45b1442f0f","7d4e351fc1b34d4f92b15bd4a925794c","9ab26ca6a6c947249830ce066c43baa3","41f4479fbac9400d9318f70b32836c27","7e50440299df43b48775319623bfb1e4"]},"id":"zDE29d0wTQKM","executionInfo":{"status":"ok","timestamp":1611742249042,"user_tz":-60,"elapsed":7215,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"dc7e96b7-06a8-46b1-a03e-9c6a15744be2"},"source":["# Instantiating the cnn-bert model with arguments \n","embed_dim = 768\n","class_num = len(mlb.classes_)\n","kernel_num = 3\n","kernel_sizes = [2, 3, 4]\n","dropout = 0.5\n","\n","# Post running this cell model will be downloaded from web (ignore errors)\n","model = simpleCNN(\n","    embed_dim=embed_dim,\n","    class_num=class_num,\n","    kernel_num=kernel_num,\n","    kernel_sizes=kernel_sizes,\n","    dropout=dropout\n",")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"32c7f2bafe3045bea07096d735699ab3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"51dda6ec43f54782a50be44c6a9ef8c3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertModel: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"WZA2g-OjIlH7"},"source":["**Define train, val, test functions**"]},{"cell_type":"code","metadata":{"id":"hoSJ2HlHTTdE"},"source":["# Training function \n","def train_fn(dataloader,model,optimizer,device):\n","  # setting the supplied to training mode \n","  model.train() \n","  losses = []\n","  f_output = []\n","  f_target = [] \n","  # loading the batchwise data to the model \n","  for d in dataloader:\n","    in_ids,token_ids,att_mask,targets = d[\"input_ids\"].to(device),d[\"token_type_ids\"].to(device),d[\"attention_mask\"].to(device),d[\"labels\"].to(device)\n","    # setting optimizer to to zero \n","    optimizer.zero_grad()\n","    # forward propogation \n","    outs = model(in_ids,token_ids,att_mask)\n","    loss = loss_fn(outs,targets)\n","    # backpropogation \n","    loss.backward()\n","    # weights update \n","    optimizer.step()\n","    # getting loss,targets,output values as return \n","    losses.append(loss.cpu().detach())\n","    # final layer values with sigmoid to get values between 0 and 1\n","    outs = torch.sigmoid(outs)\n","    f_output.extend(outs.cpu().detach().numpy())\n","    f_target.extend(targets.cpu().detach().numpy())\n","  return f_output,f_target,np.sum(losses)/len(dataloader) \n","\n","def validation_fn(dataloader,model,device):\n","    model.eval()\n","    losses = []\n","    f_output = []\n","    f_target = []\n","    # keeping the no-gradient on  \n","    with torch.no_grad():\n","      # loading the batchwise data to the model\n","      for d in dataloader:\n","        in_ids,token_ids,att_mask,targets = d[\"input_ids\"].to(device),d[\"token_type_ids\"].to(device),d[\"attention_mask\"].to(device),d[\"labels\"].to(device)\n","        # forward propogation \n","        outs = model(in_ids,token_ids,att_mask)\n","        loss = loss_fn(outs,targets)\n","        # final layer with sigmoid to get values between 0 and 1\n","        outs = torch.sigmoid(outs)\n","        # getting loss,targets,output values as return \n","        losses.append(loss.cpu().detach())\n","        f_output.extend(outs.cpu().detach().numpy())\n","        f_target.extend(targets.cpu().detach().numpy())\n","    return f_output,f_target,np.sum(losses)/len(dataloader) \n","\n","def test_fn(dataloader,model,device):\n","    model.eval()\n","    f_output = []\n","    # keeping the no-gradient on  \n","    with torch.no_grad():\n","      # loading the batchwise data to the model\n","      for d in dataloader:\n","        in_ids,token_ids,att_mask = d[\"input_ids\"].to(device),d[\"token_type_ids\"].to(device),d[\"attention_mask\"].to(device)\n","        # forward propogation \n","        outs = model(in_ids,token_ids,att_mask)\n","        # final layer with sigmoid to get values between 0 and 1\n","        outs = torch.sigmoid(outs)\n","        # getting loss,targets,output values as return \n","        f_output.extend(outs.cpu().detach().numpy())\n","    return f_output \n","\n","# This loss combines a Sigmoid layer and the BCELoss in one single class. \n","def loss_fn(out,target):\n","  return nn.BCEWithLogitsLoss()(out,target)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YPHC7paDFG8c"},"source":["**Train Model**"]},{"cell_type":"code","metadata":{"id":"OyOggOnDTiri","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611745934684,"user_tz":-60,"elapsed":2707948,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"c5aa36cb-2df1-4f77-b204-3d0b3a4ef126"},"source":["# define number of epochs\n","epochs = 20\n","from sklearn.model_selection import train_test_split\n","from eval_metric import evaluate_results\n","from sklearn.metrics import accuracy_score\n","\n","# splitting the train and test dataset \n","train,val = train_test_split(train_df,test_size=0.1)\n","\n","# instantiating dataset class with supplied data and labels \n","t_dataset = GenreDataset(train[\"description\"].values,train[mlb.classes_].values)\n","# dataset and batchwise loader from pytorch \n","t_loader = DataLoader(t_dataset,batch_size=32,shuffle=True,num_workers=4)\n","\n","v_dataset = GenreDataset(val[\"description\"].values,val[mlb.classes_].values)\n","v_loader = DataLoader(v_dataset,batch_size=32,shuffle=False,num_workers=4)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#optimizer methodology used with very low learning rate \n","optimizer = torch.optim.Adam(model.parameters(),lr=1e-5)\n","# loading the model to device for getting the weights to device \n","model.to(device)\n","best_loss = np.inf \n","print(\"TRAINING STARTED...\") \n","\n","for e in range(epochs):\n","  t_out,t_target,train_loss = train_fn(t_loader,model,optimizer,device)\n","  v_out,v_target,val_loss = validation_fn(v_loader,model,device)  \n","  # here evaluation results from the given file \n","  acc,f1,fpr,fnr = evaluate_results(np.array(t_target),np.round(t_out))\n","  acc2,f1_2,fpr2,fnr2 = evaluate_results(np.array(v_target),np.round(v_out))\n","  print(f\"{e+1}-\")\n","  if val_loss < best_loss:\n","    torch.save(model.state_dict(),\"model_bert_v2.pth\")\n","    best_loss = val_loss \n","  # uncomment below code to get the accuracy score directly from sklearn \n","  #print((accuracy_score(np.array(t_target),np.round(t_out)),accuracy_score(np.array(v_target),np.round(v_out))))\n","\n","  print(\"train_loss:\", round(train_loss,3),\"train_accuracy:\",round(acc,3), \"train_f1:\", f1, \"train_fpr:\", fpr, \"train_fnr:\", fnr) \n","  print(\"val_loss:\",round(val_loss,3),\"val_accuracy:\",round(acc2,3), \"val_f1:\", f1_2, \"val_fpr:\", fpr2, \"val_fnr:\", fnr2)\n"," "],"execution_count":null,"outputs":[{"output_type":"stream","text":["TRAINING STARTED...\n","1-\n","train_loss: 0.598 train_accuracy: 0.0 train_f1: [0.1890389197776013, 0.034364261168384876, 0.06991260923845194, 0.24751066856330015, 0.36481700118063753, 0.09146341463414635, 0.6482851378614661, 0.125, 0.1929657794676806, 0.07746031746031747, 0.17801047120418848, 0.08521303258145363] train_fpr: [0.33935907970419066, 0.08065720687079911, 0.027572624322993598, 0.08576329331046312, 0.22291904218928163, 0.3076923076923077, 0.45170876671619614, 0.1978476821192053, 0.6532951289398281, 0.5311550151975684, 0.07443116081758581, 0.07686305111197815] train_fnr: [0.6210191082802548, 0.9285714285714286, 0.9609483960948396, 0.7908653846153846, 0.6891348088531187, 0.6666666666666666, 0.3124108416547789, 0.8373493975903614, 0.3344262295081967, 0.47413793103448276, 0.7806451612903226, 0.9081081081081082]\n","val_loss: 0.562 val_accuracy: 0.0 val_f1: [0.06521739130434782, 0.0, 0.0, 0.0, 0.03278688524590164, 0.0, 0.8328267477203647, 0.0, 0.25203252032520324, 0.061224489795918366, 0.0, 0.0] val_fpr: [0.18421052631578946, 0.0, 0.0, 0.0, 0.015873015873015872, 0.04498269896193772, 0.24503311258278146, 0.0, 0.6642066420664207, 0.5929824561403508, 0.0, 0.0] val_fnr: [0.925, 1.0, 1.0, 1.0, 0.9829059829059829, 1.0, 0.11612903225806452, 1.0, 0.11428571428571428, 0.7142857142857143, 1.0, 1.0]\n","2-\n","train_loss: 0.594 train_accuracy: 0.0 train_f1: [0.1875993640699523, 0.07407407407407407, 0.09057527539779682, 0.28449502133712656, 0.4025301897642323, 0.08113590263691683, 0.6472166331321262, 0.1420643729189789, 0.20067862336403297, 0.08897243107769424, 0.18615751789976137, 0.07906976744186046] train_fpr: [0.33935907970419066, 0.09036594473487677, 0.0310192023633678, 0.08018867924528301, 0.22519954389965793, 0.310371220818982, 0.45690936106983654, 0.20902317880794702, 0.6348751534997953, 0.5353343465045592, 0.08677207867335134, 0.0889582520483808] train_fnr: [0.6242038216560509, 0.8285714285714286, 0.9483960948396095, 0.7596153846153846, 0.647887323943662, 0.7037037037037037, 0.31169757489301, 0.8072289156626506, 0.32131147540983607, 0.3879310344827586, 0.7483870967741936, 0.9081081081081082]\n","val_loss: 0.565 val_accuracy: 0.007 val_f1: [0.048192771084337345, 0.0, 0.0, 0.0, 0.4260355029585799, 0.0, 0.819047619047619, 0.0, 0.2175438596491228, 0.11206896551724137, 0.125, 0.0] val_fpr: [0.15413533834586465, 0.0, 0.0, 0.0, 0.08465608465608465, 0.03806228373702422, 0.2052980132450331, 0.0, 0.8081180811808119, 0.6947368421052632, 0.0, 0.0] val_fnr: [0.95, 1.0, 1.0, 1.0, 0.6923076923076923, 1.0, 0.16774193548387098, 1.0, 0.11428571428571428, 0.38095238095238093, 0.9333333333333333, 1.0]\n","3-\n","train_loss: 0.591 train_accuracy: 0.0 train_f1: [0.23839999999999997, 0.025889967637540454, 0.13301662707838477, 0.29329608938547486, 0.4056939501779359, 0.07975460122699385, 0.6557922769640478, 0.1664766248574686, 0.20450097847358123, 0.07275047862156987, 0.20202020202020202, 0.07834101382488479] train_fpr: [0.32333607230895645, 0.08775205377147124, 0.033973412112259974, 0.08361921097770154, 0.19954389965792474, 0.3076923076923077, 0.45839524517087665, 0.19536423841059603, 0.6262791649611134, 0.5296352583586627, 0.0775163902815272, 0.09051892313694888] train_fnr: [0.5254777070063694, 0.9428571428571428, 0.9218967921896792, 0.7475961538461539, 0.6559356136820925, 0.7111111111111111, 0.29743223965763194, 0.7801204819277109, 0.31475409836065577, 0.5086206896551724, 0.7419354838709677, 0.9081081081081082]\n","val_loss: 0.557 val_accuracy: 0.0 val_f1: [0.025974025974025976, 0.0, 0.0, 0.0, 0.016, 0.0, 0.8316831683168316, 0.0, 0.21086261980830673, 0.11224489795918367, 0.0, 0.0] val_fpr: [0.13533834586466165, 0.0, 0.0, 0.0, 0.037037037037037035, 0.06920415224913495, 0.1456953642384106, 0.0, 0.9040590405904059, 0.5754385964912281, 0.0, 0.0] val_fnr: [0.975, 1.0, 1.0, 1.0, 0.9914529914529915, 1.0, 0.1870967741935484, 1.0, 0.05714285714285714, 0.47619047619047616, 1.0, 1.0]\n","4-\n","train_loss: 0.587 train_accuracy: 0.0 train_f1: [0.23275145469659184, 0.04545454545454545, 0.2018348623853211, 0.29057591623036655, 0.3894993894993895, 0.08414872798434442, 0.674270379067427, 0.16607354685646503, 0.19530876017233123, 0.0779220779220779, 0.19900497512437812, 0.06756756756756757] train_fpr: [0.3077239112571898, 0.08625840179238238, 0.032988675529295915, 0.1016295025728988, 0.18529076396807298, 0.32300038270187525, 0.4264487369985141, 0.18253311258278146, 0.6467458043389276, 0.5182370820668692, 0.07983031237948322, 0.09520093640265315] train_fnr: [0.554140127388535, 0.9, 0.8772663877266388, 0.7331730769230769, 0.6790744466800804, 0.6814814814814815, 0.2831669044222539, 0.7891566265060241, 0.33114754098360655, 0.4827586206896552, 0.7419354838709677, 0.918918918918919]\n","val_loss: 0.559 val_accuracy: 0.003 val_f1: [0.2689075630252101, 0.0, 0.24489795918367346, 0.0, 0.0, 0.0, 0.7804878048780488, 0.32558139534883723, 0.24087591240875908, 0.11981566820276499, 0.6956521739130436, 0.0909090909090909] val_fpr: [0.23684210526315788, 0.0, 0.0, 0.0, 0.047619047619047616, 0.0726643598615917, 0.46357615894039733, 0.0, 0.7601476014760148, 0.6421052631578947, 0.0, 0.0] val_fnr: [0.6, 1.0, 0.8604651162790697, 1.0, 1.0, 1.0, 0.07096774193548387, 0.8055555555555556, 0.05714285714285714, 0.38095238095238093, 0.4666666666666667, 0.9523809523809523]\n","5-\n","train_loss: 0.585 train_accuracy: 0.002 train_f1: [0.22750424448217318, 0.0457516339869281, 0.23861171366594358, 0.2740447957839262, 0.39905548996458085, 0.06876790830945559, 0.6909677419354839, 0.17059483726150396, 0.20662382600098864, 0.08134280180761781, 0.19597989949748743, 0.0525164113785558] train_fpr: [0.2999178307313065, 0.08551157580283794, 0.04677498769079271, 0.10248713550600343, 0.2063854047890536, 0.33524684270952926, 0.4658246656760773, 0.19991721854304637, 0.6176831764224314, 0.520516717325228, 0.07867335133050521, 0.10144362075692548] train_fnr: [0.5732484076433121, 0.9, 0.8465829846582985, 0.75, 0.6599597585513078, 0.7333333333333333, 0.23609129814550642, 0.7710843373493976, 0.31475409836065577, 0.45689655172413796, 0.7483870967741936, 0.9351351351351351]\n","val_loss: 0.55 val_accuracy: 0.003 val_f1: [0.2838709677419355, 0.0, 0.15053763440860218, 0.0, 0.37333333333333335, 0.0, 0.7844155844155845, 0.05405405405405406, 0.25196850393700787, 0.1414141414141414, 0.4210526315789474, 0.0] val_fpr: [0.34962406015037595, 0.0, 0.0, 0.0, 0.026455026455026454, 0.06228373702422145, 0.5231788079470199, 0.0, 0.6900369003690037, 0.5719298245614035, 0.0, 0.0] val_fnr: [0.45, 1.0, 0.9186046511627907, 1.0, 0.7606837606837606, 1.0, 0.025806451612903226, 0.9722222222222222, 0.08571428571428572, 0.3333333333333333, 0.7333333333333333, 1.0]\n","6-\n","train_loss: 0.581 train_accuracy: 0.001 train_f1: [0.24476650563607083, 0.07096774193548386, 0.2486828240252898, 0.29198966408268734, 0.42510361160449966, 0.09818181818181819, 0.68606530876172, 0.1639344262295082, 0.21184738955823293, 0.07760381211708645, 0.28708133971291866, 0.09486166007905138] train_fpr: [0.3188167625308135, 0.08551157580283794, 0.056129985228951254, 0.10506003430531732, 0.19156214367160776, 0.34864140834290086, 0.46805349182763745, 0.1870860927152318, 0.6041751944330741, 0.49240121580547114, 0.07828769764751253, 0.11587982832618025] train_fnr: [0.5159235668789809, 0.8428571428571429, 0.8354253835425384, 0.7283653846153846, 0.6388329979879276, 0.6, 0.24322396576319544, 0.7891566265060241, 0.3081967213114754, 0.5086206896551724, 0.6129032258064516, 0.8702702702702703]\n","val_loss: 0.544 val_accuracy: 0.003 val_f1: [0.28571428571428575, 0.0, 0.15053763440860218, 0.0, 0.2876712328767123, 0.0, 0.8169014084507041, 0.19999999999999998, 0.21725239616613418, 0.10404624277456646, 0.5, 0.0909090909090909] val_fpr: [0.23308270676691728, 0.0, 0.0, 0.0, 0.042328042328042326, 0.10380622837370242, 0.36423841059602646, 0.0, 0.9003690036900369, 0.5017543859649123, 0.0, 0.0] val_fnr: [0.575, 1.0, 0.9186046511627907, 1.0, 0.8205128205128205, 1.0, 0.06451612903225806, 0.8888888888888888, 0.02857142857142857, 0.5714285714285714, 0.6666666666666666, 0.9523809523809523]\n","7-\n","train_loss: 0.58 train_accuracy: 0.0 train_f1: [0.2222222222222222, 0.05442176870748299, 0.3076923076923077, 0.20833333333333331, 0.42470588235294116, 0.08426966292134831, 0.6757874642061724, 0.16129032258064516, 0.2270606531881804, 0.0887459807073955, 0.23713646532438476, 0.047337278106508875] train_fpr: [0.32128184059161874, 0.08065720687079911, 0.04775972427375677, 0.09819897084048028, 0.1966932725199544, 0.33983926521239954, 0.5044576523031203, 0.19288079470198677, 0.5751125665165779, 0.520516717325228, 0.09217123023524874, 0.12095200936402653] train_fnr: [0.5636942675159236, 0.8857142857142857, 0.793584379358438, 0.8197115384615384, 0.6368209255533199, 0.6666666666666666, 0.24251069900142652, 0.7891566265060241, 0.2819672131147541, 0.4051724137931034, 0.6580645161290323, 0.9351351351351351]\n","val_loss: 0.54 val_accuracy: 0.007 val_f1: [0.1411764705882353, 0.0, 0.0, 0.0, 0.15503875968992248, 0.0, 0.831858407079646, 0.0, 0.21739130434782605, 0.09356725146198831, 0.23529411764705882, 0.0] val_fpr: [0.14661654135338345, 0.0, 0.0, 0.0, 0.010582010582010581, 0.15570934256055363, 0.2847682119205298, 0.0, 0.9298892988929889, 0.4982456140350877, 0.0, 0.0] val_fnr: [0.85, 1.0, 1.0, 1.0, 0.9145299145299145, 1.0, 0.09032258064516129, 1.0, 0.0, 0.6190476190476191, 0.8666666666666667, 1.0]\n","8-\n","train_loss: 0.576 train_accuracy: 0.002 train_f1: [0.24406779661016947, 0.06472491909385113, 0.30833333333333335, 0.30136986301369867, 0.423444976076555, 0.07822222222222222, 0.6738281249999999, 0.18764302059496565, 0.20679012345679013, 0.08865010073875083, 0.248062015503876, 0.11313131313131312] train_fpr: [0.29663105998356615, 0.08551157580283794, 0.04677498769079271, 0.11406518010291596, 0.1847206385404789, 0.36203597397627246, 0.47176820208023773, 0.19039735099337748, 0.5886205485059354, 0.496580547112462, 0.07096027767065176, 0.11002731174404994] train_fnr: [0.5414012738853503, 0.8571428571428571, 0.793584379358438, 0.7091346153846154, 0.6438631790744467, 0.674074074074074, 0.26176890156918686, 0.7530120481927711, 0.34098360655737703, 0.43103448275862066, 0.6903225806451613, 0.8486486486486486]\n","val_loss: 0.531 val_accuracy: 0.003 val_f1: [0.22950819672131145, 0.0, 0.0, 0.0, 0.2733812949640288, 0.0, 0.8222222222222222, 0.0, 0.22950819672131148, 0.10909090909090909, 0.33333333333333337, 0.0] val_fpr: [0.2556390977443609, 0.0, 0.0, 0.0, 0.015873015873015872, 0.08650519031141868, 0.37748344370860926, 0.0, 0.8671586715867159, 0.47368421052631576, 0.0, 0.0] val_fnr: [0.65, 1.0, 1.0, 1.0, 0.8376068376068376, 1.0, 0.04516129032258064, 1.0, 0.0, 0.5714285714285714, 0.8, 1.0]\n","9-\n","train_loss: 0.575 train_accuracy: 0.001 train_f1: [0.24006762468300932, 0.06688963210702341, 0.2928870292887029, 0.225201072386059, 0.42277407631738345, 0.099009900990099, 0.6846846846846847, 0.17298578199052134, 0.21278772378516622, 0.08278580814717477, 0.23441396508728182, 0.10060362173038231] train_fpr: [0.29868529170090385, 0.08177744585511576, 0.04874446085672083, 0.10548885077186965, 0.17559863169897377, 0.3157290470723307, 0.4769687964338782, 0.18170529801324503, 0.5902578796561605, 0.5102583586626139, 0.07674508291554184, 0.11197815060476005] train_fnr: [0.5477707006369427, 0.8571428571428571, 0.8047419804741981, 0.7980769230769231, 0.6488933601609658, 0.6296296296296297, 0.24108416547788872, 0.7801204819277109, 0.3180327868852459, 0.45689655172413796, 0.6967741935483871, 0.8648648648648649]\n","val_loss: 0.548 val_accuracy: 0.016 val_f1: [0.29752066115702475, 0.0, 0.4869565217391305, 0.0, 0.31645569620253167, 0.0, 0.7942857142857144, 0.48, 0.2524271844660194, 0.08571428571428572, 0.64, 0.0] val_fpr: [0.23684210526315788, 0.0, 0.004545454545454545, 0.0, 0.08465608465608465, 0.1384083044982699, 0.3708609271523179, 0.007407407407407408, 0.5350553505535055, 0.39649122807017545, 0.006872852233676976, 0.0035087719298245615] val_fnr: [0.55, 1.0, 0.6744186046511628, 1.0, 0.7863247863247863, 1.0, 0.1032258064516129, 0.6666666666666666, 0.2571428571428571, 0.7142857142857143, 0.4666666666666667, 1.0]\n","10-\n","train_loss: 0.573 train_accuracy: 0.001 train_f1: [0.24448217317487267, 0.0445859872611465, 0.2984455958549223, 0.232620320855615, 0.449438202247191, 0.07706766917293233, 0.672020558946354, 0.18331374853113985, 0.2143983184445612, 0.0877076411960133, 0.2260127931769723, 0.0821917808219178] train_fpr: [0.29580936729663104, 0.08849887976101568, 0.05120630231413097, 0.10506003430531732, 0.1807297605473204, 0.33983926521239954, 0.49405646359583955, 0.18253311258278146, 0.5706099058534588, 0.5026595744680851, 0.10065561126108755, 0.11900117050331642] train_fnr: [0.5414012738853503, 0.9, 0.799163179916318, 0.7908653846153846, 0.6177062374245473, 0.6962962962962963, 0.25392296718972895, 0.7650602409638554, 0.33114754098360655, 0.43103448275862066, 0.6580645161290323, 0.8864864864864865]\n","val_loss: 0.539 val_accuracy: 0.0 val_f1: [0.30337078651685395, 0.0, 0.49572649572649574, 0.0, 0.6354166666666666, 0.0, 0.7445255474452556, 0.4, 0.2304832713754647, 0.14054054054054055, 0.5714285714285715, 0.0] val_fpr: [0.41729323308270677, 0.0, 0.00909090909090909, 0.0, 0.07407407407407407, 0.10034602076124567, 0.6821192052980133, 0.0, 0.7490774907749077, 0.5298245614035088, 0.0, 0.0] val_fnr: [0.325, 1.0, 0.6627906976744186, 1.0, 0.47863247863247865, 1.0, 0.012903225806451613, 0.75, 0.11428571428571428, 0.38095238095238093, 0.6, 1.0]\n","11-\n","train_loss: 0.571 train_accuracy: 0.001 train_f1: [0.2457044673539519, 0.0694006309148265, 0.33946830265848665, 0.225201072386059, 0.45126993502658, 0.0671785028790787, 0.6800909386164339, 0.1803084223013049, 0.2274143302180686, 0.08547008547008547, 0.20187793427230047, 0.07563025210084034] train_fpr: [0.29046836483155297, 0.08812546676624347, 0.04677498769079271, 0.10548885077186965, 0.1807297605473204, 0.3337160352085725, 0.46805349182763745, 0.18004966887417218, 0.5738845681539091, 0.5091185410334347, 0.08792903972232935, 0.10651580179477176] train_fnr: [0.5445859872611465, 0.8428571428571429, 0.7684797768479776, 0.7980769230769231, 0.6156941649899397, 0.7407407407407407, 0.25320970042796004, 0.7710843373493976, 0.2819672131147541, 0.4396551724137931, 0.7225806451612903, 0.9027027027027027]\n","val_loss: 0.532 val_accuracy: 0.003 val_f1: [0.32432432432432434, 0.0, 0.3818181818181818, 0.0, 0.5517241379310346, 0.0, 0.7794871794871795, 0.10526315789473684, 0.21118012422360247, 0.08333333333333334, 0.6666666666666667, 0.0] val_fpr: [0.3157894736842105, 0.0, 0.013636363636363636, 0.0, 0.047619047619047616, 0.09342560553633218, 0.5496688741721855, 0.0, 0.933579335793358, 0.4105263157894737, 0.003436426116838488, 0.0] val_fnr: [0.4, 1.0, 0.7558139534883721, 1.0, 0.5897435897435898, 1.0, 0.01935483870967742, 0.9444444444444444, 0.02857142857142857, 0.7142857142857143, 0.4666666666666667, 1.0]\n","12-\n","train_loss: 0.569 train_accuracy: 0.001 train_f1: [0.24267782426778242, 0.05357142857142857, 0.259958071278826, 0.22397891963109356, 0.4207980652962516, 0.10121836925960638, 0.6953150242326331, 0.1807081807081807, 0.22432859399684044, 0.09645390070921986, 0.25125628140703515, 0.10062893081761005] train_fpr: [0.30238290879211177, 0.09596713965646005, 0.05563761693746923, 0.11063464837049743, 0.17787913340935005, 0.33601224646000766, 0.45839524517087665, 0.17094370860927152, 0.5652885796152272, 0.46580547112462006, 0.07443116081758581, 0.10456496293406164] train_fnr: [0.5382165605095541, 0.8714285714285714, 0.8270571827057183, 0.7956730769230769, 0.6498993963782697, 0.6, 0.23252496433666192, 0.7771084337349398, 0.3016393442622951, 0.41379310344827586, 0.6774193548387096, 0.8702702702702703]\n","val_loss: 0.525 val_accuracy: 0.007 val_f1: [0.23214285714285715, 0.0, 0.36190476190476195, 0.0, 0.3636363636363636, 0.0, 0.7988668555240793, 0.0, 0.2169491525423729, 0.08264462809917356, 0.6666666666666667, 0.0] val_fpr: [0.22180451127819548, 0.0, 0.0, 0.0, 0.0, 0.11418685121107267, 0.37748344370860926, 0.0, 0.8413284132841329, 0.3333333333333333, 0.003436426116838488, 0.0035087719298245615] val_fnr: [0.675, 1.0, 0.7790697674418605, 1.0, 0.7777777777777778, 1.0, 0.09032258064516129, 1.0, 0.08571428571428572, 0.7619047619047619, 0.4666666666666667, 1.0]\n","13-\n","train_loss: 0.563 train_accuracy: 0.001 train_f1: [0.24129141886151234, 0.06756756756756756, 0.36253776435045315, 0.2544529262086514, 0.4424352019288728, 0.07868252516010979, 0.7054639508567734, 0.14832535885167464, 0.22907949790794974, 0.08817354793561932, 0.2506142506142506, 0.0934959349593496] train_fpr: [0.2962202136400986, 0.08065720687079911, 0.047267355982274745, 0.11578044596912522, 0.16989737742303307, 0.35017221584385766, 0.4457652303120357, 0.18294701986754966, 0.5681539091281211, 0.4749240121580547, 0.0775163902815272, 0.11080764728833398] train_fnr: [0.5477707006369427, 0.8571428571428571, 0.7489539748953975, 0.7596153846153846, 0.630784708249497, 0.6814814814814815, 0.22182596291012838, 0.8132530120481928, 0.2819672131147541, 0.45689655172413796, 0.6709677419354839, 0.8756756756756757]\n","val_loss: 0.529 val_accuracy: 0.007 val_f1: [0.2363636363636364, 0.0, 0.24489795918367346, 0.0, 0.5573770491803278, 0.0, 0.8119891008174387, 0.0, 0.25641025641025633, 0.12345679012345681, 0.33333333333333337, 0.0] val_fpr: [0.21428571428571427, 0.0, 0.0, 0.0, 0.07936507936507936, 0.1695501730103806, 0.41721854304635764, 0.0, 0.6236162361623616, 0.45964912280701753, 0.0, 0.0] val_fnr: [0.675, 1.0, 0.8604651162790697, 1.0, 0.5641025641025641, 1.0, 0.03870967741935484, 1.0, 0.14285714285714285, 0.5238095238095238, 0.8, 1.0]\n","14-\n","train_loss: 0.564 train_accuracy: 0.003 train_f1: [0.2474916387959866, 0.07262569832402234, 0.3472222222222222, 0.24728260869565213, 0.44746162927981115, 0.08014911463187326, 0.6945169712793734, 0.18013856812933027, 0.2215122470713525, 0.08395396073121192, 0.251889168765743, 0.09276437847866419] train_fpr: [0.30156121610517667, 0.10268857356235997, 0.05711472181191531, 0.09819897084048028, 0.1830102622576967, 0.3425181783390739, 0.4442793462109955, 0.18874172185430463, 0.5587392550143266, 0.49354103343465044, 0.07404550713459314, 0.12836519703472493] train_fnr: [0.5286624203821656, 0.8142857142857143, 0.7559274755927475, 0.78125, 0.6187122736418511, 0.6814814814814815, 0.24108416547788872, 0.7650602409638554, 0.3180327868852459, 0.46551724137931033, 0.6774193548387096, 0.8648648648648649]\n","val_loss: 0.528 val_accuracy: 0.01 val_f1: [0.26666666666666666, 0.0, 0.5333333333333333, 0.0, 0.5294117647058824, 0.0, 0.7901907356948229, 0.19999999999999998, 0.21383647798742136, 0.19565217391304346, 0.33333333333333337, 0.0] val_fpr: [0.24060150375939848, 0.0, 0.00909090909090909, 0.01893939393939394, 0.042328042328042326, 0.1695501730103806, 0.44370860927152317, 0.0, 0.9188191881918819, 0.21754385964912282, 0.0, 0.0] val_fnr: [0.6, 1.0, 0.627906976744186, 1.0, 0.6153846153846154, 1.0, 0.06451612903225806, 0.8888888888888888, 0.02857142857142857, 0.5714285714285714, 0.8, 1.0]\n","15-\n","train_loss: 0.56 train_accuracy: 0.003 train_f1: [0.25848142164781907, 0.049046321525885554, 0.3359073359073359, 0.2480211081794195, 0.5112262521588946, 0.09671532846715329, 0.6940491591203105, 0.17715617715617715, 0.23812066203950882, 0.09388335704125178, 0.28498727735368956, 0.13602941176470587] train_fpr: [0.31388660640920296, 0.1075429424943988, 0.07139340226489414, 0.10634648370497427, 0.17046750285062715, 0.3474933027171833, 0.45839524517087665, 0.18625827814569537, 0.550552599263201, 0.46504559270516715, 0.0701889703046664, 0.12563402262973078] train_fnr: [0.49044585987261147, 0.8714285714285714, 0.7573221757322176, 0.7740384615384616, 0.5533199195171026, 0.6074074074074074, 0.2346647646219686, 0.7710843373493976, 0.26885245901639343, 0.43103448275862066, 0.6387096774193548, 0.8]\n","val_loss: 0.517 val_accuracy: 0.01 val_f1: [0.18181818181818182, 0.0, 0.06741573033707865, 0.0, 0.4313725490196078, 0.0, 0.7810026385224275, 0.0, 0.30303030303030304, 0.1568627450980392, 0.5, 0.07692307692307693] val_fpr: [0.11278195488721804, 0.0, 0.0, 0.0, 0.015873015873015872, 0.07612456747404844, 0.5033112582781457, 0.0, 0.3874538745387454, 0.5859649122807018, 0.0, 0.014035087719298246] val_fnr: [0.825, 1.0, 0.9651162790697675, 1.0, 0.717948717948718, 1.0, 0.04516129032258064, 1.0, 0.2857142857142857, 0.23809523809523808, 0.6666666666666666, 0.9523809523809523]\n","16-\n","train_loss: 0.558 train_accuracy: 0.004 train_f1: [0.2644628099173553, 0.06483790523690773, 0.3323383084577114, 0.27868852459016397, 0.5097365406643757, 0.07272727272727271, 0.7016861219195849, 0.17321016166281755, 0.22466960352422907, 0.09665955934612651, 0.2935779816513761, 0.1348314606741573] train_fpr: [0.30238290879211177, 0.11874533233756535, 0.05957656326932546, 0.09176672384219554, 0.1750285062713797, 0.3337160352085725, 0.4457652303120357, 0.18998344370860928, 0.5349979533360623, 0.46466565349544076, 0.08368684920940996, 0.15450643776824036] train_fnr: [0.49044585987261147, 0.8142857142857143, 0.7670850767085077, 0.7548076923076923, 0.5523138832997988, 0.7185185185185186, 0.2282453637660485, 0.7740963855421686, 0.33114754098360655, 0.41379310344827586, 0.5870967741935483, 0.772972972972973]\n","val_loss: 0.519 val_accuracy: 0.016 val_f1: [0.25, 0.0, 0.3925233644859813, 0.0, 0.28378378378378377, 0.0, 0.7768115942028985, 0.0, 0.2251655629139073, 0.1512605042016807, 0.125, 0.0] val_fpr: [0.16541353383458646, 0.0, 0.0, 0.0, 0.05291005291005291, 0.1384083044982699, 0.3708609271523179, 0.0, 0.8597785977859779, 0.312280701754386, 0.0, 0.0] val_fnr: [0.7, 1.0, 0.7558139534883721, 1.0, 0.8205128205128205, 1.0, 0.13548387096774195, 1.0, 0.02857142857142857, 0.5714285714285714, 0.9333333333333333, 1.0]\n","17-\n","train_loss: 0.554 train_accuracy: 0.005 train_f1: [0.26823134953897737, 0.09448818897637795, 0.3669902912621359, 0.23860589812332442, 0.4611792177466433, 0.08767123287671233, 0.6978253813696852, 0.20023557126030625, 0.24746906636670415, 0.09586056644880174, 0.28433734939759037, 0.153125] train_fpr: [0.2953985209531635, 0.10941000746825989, 0.06105366814377154, 0.10334476843910806, 0.1847206385404789, 0.34902411021814006, 0.4487369985141159, 0.17880794701986755, 0.5128939828080229, 0.4540273556231003, 0.0775163902815272, 0.15840811548966055] train_fnr: [0.49044585987261147, 0.7428571428571429, 0.7364016736401674, 0.7860576923076923, 0.60261569416499, 0.6444444444444445, 0.2332382310984308, 0.7439759036144579, 0.2786885245901639, 0.43103448275862066, 0.6193548387096774, 0.7351351351351352]\n","val_loss: 0.517 val_accuracy: 0.01 val_f1: [0.30985915492957744, 0.0, 0.3584905660377359, 0.0, 0.6363636363636364, 0.0, 0.7780678851174936, 0.19999999999999998, 0.26666666666666666, 0.16551724137931034, 0.33333333333333337, 0.12121212121212123] val_fpr: [0.3007518796992481, 0.0, 0.004545454545454545, 0.0, 0.09523809523809523, 0.0657439446366782, 0.5231788079470199, 0.0, 0.5424354243542435, 0.3929824561403509, 0.0, 0.03508771929824561] val_fnr: [0.45, 1.0, 0.7790697674418605, 1.0, 0.46153846153846156, 1.0, 0.03870967741935484, 0.8888888888888888, 0.2, 0.42857142857142855, 0.8, 0.9047619047619048]\n","18-\n","train_loss: 0.553 train_accuracy: 0.003 train_f1: [0.2506307821698907, 0.07881773399014777, 0.36506378802747796, 0.24299065420560748, 0.4974212034383954, 0.08007626310772165, 0.6962769431743958, 0.18947368421052635, 0.2528473804100228, 0.09357541899441342, 0.27250608272506077, 0.14506172839506173] train_fpr: [0.2982744453574363, 0.11949215832710978, 0.05711472181191531, 0.10377358490566038, 0.1807297605473204, 0.3337160352085725, 0.4413075780089153, 0.18294701986754966, 0.5030699959066721, 0.4745440729483283, 0.07713073659853452, 0.16230979321108077] train_fnr: [0.5254777070063694, 0.7714285714285715, 0.7405857740585774, 0.78125, 0.5633802816901409, 0.6888888888888889, 0.2396576319543509, 0.7560240963855421, 0.2721311475409836, 0.4224137931034483, 0.6387096774193548, 0.745945945945946]\n","val_loss: 0.516 val_accuracy: 0.02 val_f1: [0.12307692307692307, 0.0, 0.20833333333333334, 0.0, 0.3624161073825503, 0.0, 0.7484662576687118, 0.0, 0.22727272727272727, 0.14893617021276595, 0.33333333333333337, 0.0909090909090909] val_fpr: [0.07894736842105263, 0.0, 0.0, 0.0, 0.026455026455026454, 0.1245674740484429, 0.32450331125827814, 0.0, 0.7343173431734318, 0.23157894736842105, 0.0, 0.0] val_fnr: [0.9, 1.0, 0.8837209302325582, 1.0, 0.7692307692307693, 1.0, 0.2129032258064516, 1.0, 0.14285714285714285, 0.6666666666666666, 0.8, 0.9523809523809523]\n","19-\n","train_loss: 0.551 train_accuracy: 0.003 train_f1: [0.251067463706234, 0.0636604774535809, 0.42924086223055297, 0.25782227784730916, 0.5061224489795918, 0.07560756075607561, 0.6799607714939523, 0.15808383233532933, 0.2548476454293629, 0.09876543209876543, 0.32545931758530183, 0.18181818181818182] train_fpr: [0.29170090386195563, 0.11015683345780433, 0.05957656326932546, 0.12006861063464837, 0.1636259977194983, 0.35744355147340223, 0.45839524517087665, 0.1808774834437086, 0.5198526401964797, 0.4532674772036474, 0.06324720401079831, 0.17362465860319937] train_fnr: [0.5318471337579618, 0.8285714285714286, 0.6806136680613668, 0.7524038461538461, 0.5633802816901409, 0.6888888888888889, 0.2582025677603424, 0.8012048192771084, 0.2459016393442623, 0.41379310344827586, 0.6, 0.6594594594594595]\n","val_loss: 0.516 val_accuracy: 0.02 val_f1: [0.28571428571428575, 0.1818181818181818, 0.46017699115044247, 0.0, 0.31788079470198677, 0.0, 0.7732558139534884, 0.19999999999999998, 0.26548672566371684, 0.12949640287769787, 0.125, 0.11428571428571427] val_fpr: [0.14285714285714285, 0.016611295681063124, 0.004545454545454545, 0.0, 0.05291005291005291, 0.1384083044982699, 0.3708609271523179, 0.0, 0.5940959409594095, 0.3824561403508772, 0.0, 0.042105263157894736] val_fnr: [0.675, 0.8, 0.6976744186046512, 1.0, 0.7948717948717948, 1.0, 0.14193548387096774, 0.8888888888888888, 0.14285714285714285, 0.5714285714285714, 0.9333333333333333, 0.9047619047619048]\n","20-\n","train_loss: 0.547 train_accuracy: 0.003 train_f1: [0.2654424040066778, 0.0979381443298969, 0.3498542274052478, 0.23497267759562843, 0.4645390070921986, 0.0950533462657614, 0.7021346469622332, 0.16236162361623616, 0.24958217270194988, 0.09659090909090907, 0.24671916010498685, 0.1636661211129296] train_fpr: [0.29786359901396875, 0.11165048543689321, 0.06499261447562776, 0.09862778730703259, 0.17388825541619157, 0.3241484883275928, 0.4264487369985141, 0.17177152317880795, 0.5182153090462546, 0.46504559270516715, 0.06903200925568839, 0.14670308232539991] train_fnr: [0.49363057324840764, 0.7285714285714285, 0.7489539748953975, 0.7932692307692307, 0.6046277665995976, 0.6370370370370371, 0.23751783166904422, 0.8012048192771084, 0.26557377049180325, 0.41379310344827586, 0.6967741935483871, 0.7297297297297297]\n","val_loss: 0.504 val_accuracy: 0.029 val_f1: [0.31496062992125984, 0.0, 0.37735849056603776, 0.0, 0.4662576687116564, 0.0, 0.7967914438502673, 0.0, 0.29032258064516125, 0.1836734693877551, 0.4210526315789474, 0.1395348837209302] val_fpr: [0.2518796992481203, 0.009966777408637873, 0.0, 0.0, 0.042328042328042326, 0.09688581314878893, 0.46357615894039733, 0.0, 0.4575645756457565, 0.23859649122807017, 0.0, 0.06666666666666667] val_fnr: [0.5, 1.0, 0.7674418604651163, 1.0, 0.6752136752136753, 1.0, 0.03870967741935484, 1.0, 0.22857142857142856, 0.5714285714285714, 0.7333333333333333, 0.8571428571428571]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OYsoiZReFtof"},"source":["**Inference - Model1**"]},{"cell_type":"code","metadata":{"id":"sNo5oolUF4pO"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GR-MCh6NToF3","executionInfo":{"status":"ok","timestamp":1611745990184,"user_tz":-60,"elapsed":55090,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"35c42a15-8138-4704-b58e-1822fe8fbabc"},"source":["from torch.utils.data import Dataset,DataLoader\n","tokenizer_class = transformers.BertTokenizer\n","pretrained_weights='distilbert-base-uncased'\n","\n","class GenreTestDataset(Dataset):\n","  \"\"\"Dataset class with pretrained-embeddings from the distil-bert model\n","     ARGS: \n","          description: total list of text description \n","          title: total list of titles\n","          labels: one-hot encoded labels \n","     Attributes:\n","          tokenizer: Tokenizing the text and embedding with ids.\n","          max_seq: maximum number of word to consider and truncate \n","                   if there are more or pad with [0] they are less.\n","          \n","     Abreviations:\n","     N: Batch size\n","     Ci: input Channel size, it is 1 here \n","     W: words size\n","     D: embedding size\n","     Co: output channel  \n","     Ks: Kernel size\n","     C: classes\n","  \"\"\"\n","  def __init__(self,description):\n","    self.description = description\n","    self.max_seq = 250\n","    self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n","  \n","  def __len__(self):\n","    return len(self.description)\n","  \n","  def __getitem__(self,idx): \n","    # convert each text input to  string without gaps and join they before tokenizing\n","    description = \"\".join(self.description[idx].split(\" \")) \n","    inputs = self.tokenizer(description, add_special_tokens=True,truncation=True,max_length=self.max_seq)\n","    # here input_ids means the token numbers given by the embeddings\n","    input_ids = inputs[\"input_ids\"]\n","    # token_type_ids ususally helpful if we are using seperate two text data rather than as single text data \n","    token_type_ids = inputs[\"token_type_ids\"]\n","    # attention_mask will have 1:attending word and 0:padded word\n","    attention_mask = inputs[\"attention_mask\"]\n","    # here padding with [0] if the tokens are less than the max_seq\n","    input_ids = input_ids + [0] * (self.max_seq - len(input_ids))\n","    token_type_ids = token_type_ids + [0] * (self.max_seq - len(token_type_ids))\n","    attention_mask = attention_mask + [0] * (self.max_seq - len(attention_mask))\n","    return {\n","        \"input_ids\": torch.tensor(input_ids,dtype=torch.long),\n","        \"token_type_ids\": torch.tensor(token_type_ids,dtype=torch.long),\n","        \"attention_mask\": torch.tensor(attention_mask,dtype=torch.long)\n","    }\n","#test_df = pd.read_csv(\"dataset/test.csv\") \n","test_dataset = GenreTestDataset(test_df[\"description\"].values)\n","test_loader = DataLoader(test_dataset,batch_size=32,shuffle=False,num_workers=4)\n","#Instantiating the cnn-bert model with arguments \n","embed_dim = 768\n","class_num = len(mlb.classes_)\n","kernel_num = 3\n","kernel_sizes = [2, 3, 4]\n","dropout = 0.5\n","\n","#after running this cell model will be downloaded from web (ignore errors)\n","model = simpleCNN(\n","    embed_dim=embed_dim,\n","    class_num=class_num,\n","    kernel_num=kernel_num,\n","    kernel_sizes=kernel_sizes,\n","    dropout=dropout)\n","model.load_state_dict(torch.load(\"model_bert_v2.pth\", map_location=device))\n","model.to(device) \n","model.eval()\n","test_results = test_fn(test_loader,model,device)\n","\n","# this function will take list of sigmoid values as input, round it to 1/0\n","# gets the indices of value 1 or get the index of maximum sigmoid value and return corresponding labels.\n","def get_labels(result_labels):\n","  result_indices = np.where(np.round(result_labels)==1.0)[1]\n","  if len(result_indices)==0:\n","    genre = mlb.classes_[np.argmax(result_labels)]\n","    return genre\n","  else:\n","    genres = mlb.classes_[np.where(np.round(result_labels)==1.0)[1]]\n","    return genres\n","\n","test_df[\"genres\"] = test_results\n","test_df[\"genres\"] = test_df[\"genres\"].apply(lambda x: get_labels([x]))\n","test_df.to_csv(\"pred_Model1_uni_f.csv\",index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertModel: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"J1o-ka7fH74K"},"source":["**Prediction - Model1**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vx1ecU0NIA9U","executionInfo":{"status":"ok","timestamp":1611745991232,"user_tz":-60,"elapsed":56129,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"6a69e1f6-70a5-4deb-9ffb-80ee2ef2a9d9"},"source":["description = \"Set nearly a decade after the finale of the original series, this revival follows Lorelai, Rory and Emily Gilmore through four seasons of change.\"\n","\n","test = GenreTestDataset([description,])\n","test_loader = DataLoader(test,batch_size=1,shuffle=False)\n","result_labels = test_fn(test_loader,model,device) \n","#showing the result which has higest confidence in genres\n","result_indices = np.where(np.round(result_labels)==1.0)[1]\n","if len(result_indices)==0:\n","  print(mlb.classes_[np.argmax(result_labels)])\n","else:\n","  print(mlb.classes_[np.where(np.round(result_labels)==1.0)[1]])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Horror']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"__AJITE3PZwq"},"source":["### Model 2: LSTM Model with BERT pre-trained tokenizers"]},{"cell_type":"markdown","metadata":{"id":"_K9vCPZSPjRn"},"source":["**Define the Architecture**"]},{"cell_type":"code","metadata":{"id":"M4CuB9a-Pfgr"},"source":["class GenreLSTM(nn.Module):\n","    \n","    # define all the layers used in model\n","    def __init__(self,embedding_dim, hidden_dim, classes, n_layers=2, \n","                 bidirectional=True, dropout=0.2):\n","        \n","        #Constructor\n","        super().__init__()          \n","        \n","        self.bert_model = transformers.BertModel.from_pretrained(pretrained_weights)\n","        #lstm layer\n","        self.lstm = nn.LSTM(embedding_dim, \n","                           hidden_dim, \n","                           num_layers=n_layers, \n","                           bidirectional=bidirectional, \n","                           dropout=dropout,\n","                           batch_first=True)\n","        \n","        #dense layer \n","        self.fc = nn.Linear(hidden_dim * 2, classes)\n","\n","        \n","    def forward(self, ids,att,token):\n","        \n","        # text = [batch size,sent_length]\n","        embedded = self.bert_model(ids,att,token)[0]\n","        # embedded = [batch size, sent_len, emb dim]\n","\n","        packed_output, (hidden, cell) = self.lstm(embedded)\n","        #hidden = [batch size, num layers * num directions,hid dim]\n","        #cell = [batch size, num layers * num directions,hid dim]\n","        \n","        #concat the final forward and backward hidden state\n","        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n","                \n","        #hidden = [batch size, hid dim * num directions]\n","        outputs=self.fc(hidden)\n","        \n","        return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IjIeXFT_QM4H"},"source":["**Train Model**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["e13907f86b974269adfa64d9b6c0b652","40bf8eea7a48461483ad10a0bdeb03a9","cc55ec9dc0814a39958db6c16144d098","1e2128fc934e495bbe8db71e2ea972b4","4016ca956a7e48308536add21e0ea90b","3f5a6b76833649a29251c060ae9d06b0","cb478a3237b6400897c2ef375a498bec","73ffb12fb670499e8c8b66d20af458be","0740d5aa8cde4364aa615a43df05fe48","49a5da75680e42b880bfb7e77cdf1f84","26ce0a1615aa4dab8f3724cfed34e1ee","d4b3adc554df4d1bbd1940f81c9963ce","7ebd858fd0a5469c9ca2160b197935a1","8895f07fb5fd47ad80f90668fb4ae65a","84c509cbc6de48b08a703f218de670b8","8a70b54482984839bf653ca7763ba091","6f78101f68ab49e99cd368580c895088","5fe304905a184667a2954d3acc56d4a4","19b11bf4ed7c43b0bbbdf582a8725237","6c13410262964d218c4bd634bf164811","85972ac0c208406d888b6bda79adf301","0aa7e03535f2430ea833ff41e7a2fafc","1cc6c0c49df949de85f7e785e2cd8928","44adc367ed1a472a898fb84609faae8c"]},"id":"eKmeDNOTQNS5","executionInfo":{"status":"ok","timestamp":1611660562459,"user_tz":-60,"elapsed":1545971,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"fc3eff99-33d8-4940-ec59-132c4e7849e9"},"source":["epochs = 20\n","from sklearn.model_selection import train_test_split \n","from eval_metric import evaluate_results\n","from sklearn.metrics import accuracy_score\n","\n","train,val = train_test_split(train_df,test_size=0.1)\n","\n","t_dataset = GenreDataset(train[\"description\"].values,train[mlb.classes_].values)\n","t_loader = DataLoader(t_dataset,batch_size=32,shuffle=True,num_workers=4)\n","\n","v_dataset = GenreDataset(val[\"description\"].values,val[mlb.classes_].values)\n","v_loader = DataLoader(v_dataset,batch_size=32,shuffle=False,num_workers=4)\n","\n","model = GenreLSTM(768,100,len(mlb.classes_))\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","optimizer = torch.optim.Adam(model.parameters(),lr=1e-5)\n","best_loss = np.inf \n","model.to(device) \n","print(\"TRAINING STARTED...\")\n","\n","for e in range(epochs):\n","  t_out,t_target,train_loss = train_fn(t_loader,model,optimizer,device)\n","  v_out,v_target,val_loss = validation_fn(v_loader,model,device)  \n","  acc,f1,fpr,fnr = evaluate_results(np.array(t_target),np.round(t_out))\n","  acc2,f1_2,fpr2,fnr2 = evaluate_results(np.array(v_target),np.round(v_out))\n","  print(f\"{e+1}-\")\n","  #save the model as .pth if the loss is less than the earlier epochs \n","  if val_loss < best_loss:\n","    torch.save(model.state_dict(),\"model_lstm_bert_v2.pth\")\n","    best_loss = val_loss\n","  #print((accuracy_score(np.array(t_target),np.round(t_out)),accuracy_score(np.array(v_target),np.round(v_out))))\n","  print(\"train_loss:\", round(train_loss,3),\"train_accuracy:\",round(acc,3), \"train_f1:\", f1, \"train_fpr:\", fpr, \"train_fnr:\", fnr) \n","  print(\"val_loss:\",round(val_loss,3),\"val_accuracy:\",round(acc2,3), \"val_f1:\", f1_2, \"val_fpr:\", fpr2, \"val_fnr:\", fnr2)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e13907f86b974269adfa64d9b6c0b652","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0740d5aa8cde4364aa615a43df05fe48","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6f78101f68ab49e99cd368580c895088","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertModel: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["TRAINING STARTED...\n","1-\n","train_loss: 0.513 train_accuracy: 0.019 train_f1: [0.0, 0.019801980198019802, 0.023653088042049932, 0.013761467889908256, 0.0, 0.013513513513513514, 0.5809178743961353, 0.016129032258064516, 0.0, 0.0, 0.0, 0.014814814814814814] train_fpr: [0.0, 0.011202389843166542, 0.017232890201871, 0.01152860802732707, 0.0, 0.004592422502870264, 0.7027632561613144, 0.011632737847943497, 0.000819000819000819, 0.0011446012972148034, 0.0038535645472061657, 0.03238392508778775] train_fnr: [1.0, 0.9857142857142858, 0.9874476987447699, 0.9926108374384236, 1.0, 0.9925925925925926, 0.31724627395315824, 0.9912023460410557, 1.0, 1.0, 1.0, 0.9891891891891892]\n","val_loss: 0.454 val_accuracy: 0.023 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6519823788546255, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","2-\n","train_loss: 0.439 train_accuracy: 0.028 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6594999999999999, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9499626587005228, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.063875088715401, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.414 val_accuracy: 0.023 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6519823788546255, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","3-\n","train_loss: 0.407 train_accuracy: 0.026 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6697857670524501, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9648991784914115, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0347764371894961, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.392 val_accuracy: 0.023 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6519823788546255, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","4-\n","train_loss: 0.39 train_accuracy: 0.023 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6110484780157835, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7879014189693802, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.23066004258339248, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.38 val_accuracy: 0.023 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6519823788546255, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","5-\n","train_loss: 0.38 train_accuracy: 0.027 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6693246947420882, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.941747572815534, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0468417317246274, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.373 val_accuracy: 0.023 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6519823788546255, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","6-\n","train_loss: 0.374 train_accuracy: 0.023 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6428571428571429, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.806572068707991, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.16323633782824698, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.369 val_accuracy: 0.023 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6519823788546255, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","7-\n","train_loss: 0.371 train_accuracy: 0.025 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6476738107684266, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8797610156833457, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12065294535131299, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.367 val_accuracy: 0.023 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6519823788546255, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","8-\n","train_loss: 0.369 train_accuracy: 0.025 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6393659180977543, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8707991038088125, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.14123491838183108, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.365 val_accuracy: 0.023 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6519823788546255, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","9-\n","train_loss: 0.368 train_accuracy: 0.026 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6615027110766847, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.883495145631068, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0908445706174592, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.365 val_accuracy: 0.023 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6519823788546255, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","10-\n","train_loss: 0.367 train_accuracy: 0.024 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.636021505376344, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.842419716206124, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1603974449964514, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.364 val_accuracy: 0.023 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6519823788546255, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","11-\n","train_loss: 0.366 train_accuracy: 0.025 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6365338325755551, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8513816280806572, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1554293825408091, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.363 val_accuracy: 0.023 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6519823788546255, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","12-\n","train_loss: 0.366 train_accuracy: 0.026 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6569080636877247, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.90067214339059, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.09226401703335699, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.363 val_accuracy: 0.023 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6519823788546255, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","13-\n","train_loss: 0.366 train_accuracy: 0.023 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.596745027124774, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6863330843913368, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.29737402413058905, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.363 val_accuracy: 0.023 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6519823788546255, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","14-\n","train_loss: 0.365 train_accuracy: 0.025 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6549369747899159, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8603435399551904, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11497515968772179, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.362 val_accuracy: 0.02 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6122448979591837, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7848101265822784, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1891891891891892, 1.0, 1.0, 1.0, 1.0, 1.0]\n","15-\n","train_loss: 0.364 train_accuracy: 0.02 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5899102445063448, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.648991784914115, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.32363378282469835, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.361 val_accuracy: 0.023 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6518847006651884, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9873417721518988, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.006756756756756757, 1.0, 1.0, 1.0, 1.0, 1.0]\n","16-\n","train_loss: 0.362 train_accuracy: 0.019 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5873581847649919, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5750560119492159, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.3569907735982967, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.36 val_accuracy: 0.016 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.511326860841424, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5189873417721519, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.46621621621621623, 1.0, 1.0, 1.0, 1.0, 1.0]\n","17-\n","train_loss: 0.359 train_accuracy: 0.019 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6095936042638241, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5063480209111277, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.35060326472675657, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.36 val_accuracy: 0.02 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6328502415458938, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8544303797468354, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.11486486486486487, 1.0, 1.0, 1.0, 1.0, 1.0]\n","18-\n","train_loss: 0.355 train_accuracy: 0.021 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6154864328259431, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5100821508588499, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33995741660752304, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.358 val_accuracy: 0.023 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6195652173913043, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6708860759493671, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22972972972972974, 1.0, 1.0, 1.0, 1.0, 1.0]\n","19-\n","train_loss: 0.35 train_accuracy: 0.021 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6505324298160697, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5093353248693054, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.28459900638750885, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.356 val_accuracy: 0.016 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4887218045112782, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33544303797468356, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5608108108108109, 1.0, 1.0, 1.0, 1.0, 1.0]\n","20-\n","train_loss: 0.344 train_accuracy: 0.024 train_f1: [0.0, 0.0, 0.0, 0.0, 0.01384767556874382, 0.0, 0.6737864077669903, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.002288329519450801, 0.0, 0.47796863330843914, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 0.993, 1.0, 0.26117814052519517, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.355 val_accuracy: 0.039 val_f1: [0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.6054054054054054, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.6962025316455697, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 0.8378378378378378, 1.0, 0.24324324324324326, 1.0, 1.0, 1.0, 1.0, 1.0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oHUY4t84Q91z"},"source":["**Inference - Model2**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ATgAu9wQ_dp","executionInfo":{"status":"ok","timestamp":1611660838686,"user_tz":-60,"elapsed":32021,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"089a0296-197e-4167-e2f0-3cdc0131065c"},"source":["from torch.utils.data import Dataset,DataLoader\n","tokenizer_class = transformers.BertTokenizer\n","pretrained_weights='distilbert-base-uncased'\n","class LSTMTestDataset(Dataset):\n","  \"\"\"Test Dataset class with pretrained-embeddings from the distil-bert model.\n","     ARGS: \n","          description: total list of text description \n","          title: total list of titles\n","          labels: one-hot encoded labels \n","     Attributes:\n","          tokenizer: Tokenizing the text and embedding with ids.\n","          max_seq: maximum number of word to consider and truncate \n","                   if there are more or pad with [0] they are less.\n","          \n","     Abreviations:\n","     N: Batch size\n","     Ci: input Channel size, it is 1 here \n","     W: words size\n","     D: embedding size\n","     Co: output channel  \n","     Ks: Kernel size\n","     C: classes\n","  \"\"\"\n","  def __init__(self,description):\n","    self.description = description\n","    self.max_seq = 250\n","    self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n","  \n","  def __len__(self):\n","    return len(self.description)\n","  \n","  def __getitem__(self,idx): \n","    #convert each text input to  string without gaps and join they before tokenizing. \n","    description = \"\".join(self.description[idx].split(\" \")) \n","    inputs = self.tokenizer(description, add_special_tokens=True,truncation=True,max_length=self.max_seq)\n","    # here input_ids means the token numbers given by the embeddings\n","    input_ids = inputs[\"input_ids\"]\n","    # token_type_ids ususally helpful if we are using seperate two text data rather than as single text data \n","    token_type_ids = inputs[\"token_type_ids\"]\n","    # attention_mask will have 1:attending word and 0:padded word\n","    attention_mask = inputs[\"attention_mask\"]\n","    #here padding with [0] if the tokens are less than the max_seq\n","    input_ids = input_ids + [0] * (self.max_seq - len(input_ids))\n","    token_type_ids = token_type_ids + [0] * (self.max_seq - len(token_type_ids))\n","    attention_mask = attention_mask + [0] * (self.max_seq - len(attention_mask))\n","    return {\n","        \"input_ids\": torch.tensor(input_ids,dtype=torch.long),\n","        \"token_type_ids\": torch.tensor(token_type_ids,dtype=torch.long),\n","        \"attention_mask\": torch.tensor(attention_mask,dtype=torch.long)\n","    }\n","#test = pd.read_csv(\"dataset/test.csv\") \n","test_dataset = LSTMTestDataset(test_df[\"description\"].values)\n","test_loader = DataLoader(test_dataset,batch_size=32,shuffle=False,num_workers=4)\n","#Instantiating the cnn-bert model with arguments \n","#instantiate the model\n","model = GenreLSTM(768,100,len(mlb.classes_))\n","# load the model to instantiated model\n","model.load_state_dict(torch.load(\"model_lstm_bert_v2.pth\"))\n","model.to(device) \n","test_results = test_fn(test_loader,model,device)\n","\n","# this function will take list of sigmoid values as input, round it to 1/0\n","# gets the indices of value 1 or get the index of maximum sigmoid value and return corresponding labels.\n","def get_labels(result_labels):\n","  result_indices = np.where(np.round(result_labels)==1.0)[1]\n","  if len(result_indices)==0:\n","    genre = mlb.classes_[np.argmax(result_labels)]\n","    return genre\n","  else:\n","    genres = mlb.classes_[np.where(np.round(result_labels)==1.0)[1]]\n","    return genres\n","\n","test_df[\"genres\"] = test_results\n","test_df[\"genres\"] = test_df[\"genres\"].apply(lambda x: get_labels([x]))\n","test_df.to_csv(\"pred_Model2_uni_f.csv\",index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing BertModel: ['distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"rCnZmIcdRsf7"},"source":["**Prediction - Model2**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MmnJYp5CRtAi","executionInfo":{"status":"ok","timestamp":1611660717936,"user_tz":-60,"elapsed":2007,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"ee44dd08-eb23-423b-d2d8-1362e3a180fd"},"source":["# get the description and title as string  \n","description = \"After contracting HIV from a tainted blood treatment, teenaged hemophiliac Ryan White is forced to fight for his right to attend public school.\"\n","\n","test = LSTMTestDataset([description,])\n","test_loader = DataLoader(test,batch_size=1,shuffle=False)\n","result_labels = test_fn(test_loader,model,device)\n","# show the results from the labels-names, which are rounded to 1\n","result_indices = np.where(np.round(result_labels)==1.0)[1]\n","if len(result_indices)==0:\n","  print(mlb.classes_[np.argmax(result_labels)])\n","else:\n","  print(mlb.classes_[np.where(np.round(result_labels)==1.0)[1]])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Action' 'Anime' 'International' 'Sci-Fi & Fantasy' 'Thriller']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8ub8JKNqR_73"},"source":["### Model 3: GPT-2 model with LSTM using GPT-2 pre-trained tokenizers"]},{"cell_type":"markdown","metadata":{"id":"RQH1jotXSNq8"},"source":["**Dataset Module**"]},{"cell_type":"code","metadata":{"id":"hK0GaDYLSR-U"},"source":["from transformers import GPT2Tokenizer\n","\n","class GPTDataset(Dataset):\n","  \"\"\" This is dataset module for gpt and \n","      everything is similar to the earliar dataset modules seen\n","      Except the tokenizer and token_ids are not used \n","      Tokenizer used here is GPT2  \n","  \"\"\"\n","  def __init__(self,description,labels):\n","    self.description = description\n","    self.labels = labels \n","    self.max_seq = 250\n","    self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","  \n","  def __len__(self):\n","    return len(self.description)\n","  \n","  def __getitem__(self,idx): \n","    description = \"\".join(self.description[idx].split()) \n","    labels = self.labels[idx,:]\n","    inputs = self.tokenizer(description, add_special_tokens=True,truncation=True,max_length=self.max_seq)\n","    input_ids = inputs[\"input_ids\"]\n","    token_type_ids = [0,]\n","    attention_mask = inputs[\"attention_mask\"]\n","    input_ids = input_ids + [0] * (self.max_seq - len(input_ids))\n","    token_type_ids = token_type_ids + [0] * (self.max_seq - len(token_type_ids))\n","    attention_mask = attention_mask + [0] * (self.max_seq - len(attention_mask))\n","    return {\n","        \"input_ids\": torch.tensor(input_ids,dtype=torch.long),\n","        \"token_type_ids\": torch.tensor(token_type_ids,dtype=torch.long),\n","        \"attention_mask\": torch.tensor(attention_mask,dtype=torch.long),\n","        \"labels\": torch.tensor(labels,dtype=torch.float)\n","    } "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zjQtXq5ybsX5"},"source":["**Define the Architecture**"]},{"cell_type":"code","metadata":{"id":"OhkUPCePbvxM"},"source":["from transformers import GPT2Tokenizer, GPT2Model\n","class GPTModel(nn.Module):\n","    \"\"\"Module with GPT2 model and pretrained from transformers-huggingface.\n","     ARGS: \n","          embedding_dim: embedding dimention is 768 even for gpt models\n","          class_num: ouput label classes \n","          hidden_dim: hidden dimentions for lstm models\n","    \"\"\"\n","    # define all the layers used in model\n","    def __init__(self,embedding_dim,hidden_dim, classes):\n","        \n","        # Constructor\n","        super().__init__()          \n","        # pre-trained gpt2 model, transfer learning \n","        self.model = GPT2Model.from_pretrained('gpt2')  \n","        # lstm layer with 2-layers \n","        self.lstm = nn.LSTM(embedding_dim, \n","                           hidden_dim, \n","                           num_layers=2, \n","                           bidirectional=True, \n","                           dropout=0.2,\n","                           batch_first=True)     \n","        # dense layer\n","        self.fc = nn.Linear(hidden_dim*2, classes)\n","\n","        \n","    def forward(self, in_ids,token,att_mask):\n","        \n","        #text = [batch size,sent_length]\n","        embedded = self.model(input_ids=in_ids,attention_mask=att_mask)[0]              \n","        packed_output, (hidden, cell) = self.lstm(embedded)\n","        #hidden = [batch size, num layers * num directions,hid dim]\n","        #cell = [batch size, num layers * num directions,hid dim]\n","        \n","        #concat the final forward and backward hidden state\n","        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1) \n","                \n","        #hidden = [batch size, hid dim * num directions]\n","        outputs=self.fc(hidden)\n","        return outputs "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"unYAZxMtcJne"},"source":["**Train Model**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["370971a7ab2540ec802652b374834fc5","8347b927802e484b9f7a280c0b18dc6a","a430364789e2457ab9b673391e4b3242","a0867a3830814088becaa4b49b6046ec","411f54112eb64038884cfbe4211de785","31e61dfb88494173894755a41ac68ae2","e340fc19f55246a2b113107daf29dd5f","185fd7445b644f109ba78cd75805e446","19c6b3cdeb674380b4fa56693b4b7ca7","9950ce2dd4e449fca1ce7b8899ea6dbe","12cf21241b894c65bc15960a3630a190","91abf79295b343b7b56804c345b1db50","7eb122534940440db7f25ef34853d4c9","191a18f2d68c44f9934a2dcabaa87b66","3ed066e3702a4f5eb0626472786b3f01","ff3eb8a34f384336a7e15968aac270a1","a0188906e4434ffd9fa59eacc1d1df95","26c0c479f0b84991bde56c2ac38858c5","7d15d276ec604c738097a7e2f642c596","a83a9b71e71c454cbc6434eee53d3b2f","e6880d7665d84a08bee7899c61804f08","373f84344e9343a59e3bcd704863277f","5656988ad62e470e80413b2aa18db6a8","c4f44b2efa39435b8196d517441701f0","6332cfa1bc4c4b79864da45dc0e0a08a","1bca82410ab54f6d9527b0c39fe81d62","a840abb58cf34298bebacdd128f1fc1c","3ea31b45e49341b0b37b0cca09ba04ac","570eea3d9bdf4512ad77c2087c129001","4802d50e116647c19e49389dad1c7786","b97bbb06082d49449ccb241fa17c26af","3de954b0fb804ea09b4a77cf71ad7475"]},"id":"RFU2Js-Gbv0i","executionInfo":{"status":"ok","timestamp":1611657432285,"user_tz":-60,"elapsed":3314100,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"8a4eb72d-1c82-4975-a1e3-833c8d10ee9c"},"source":["epochs = 20\n","from sklearn.model_selection import train_test_split\n","from eval_metric import evaluate_results\n","from sklearn.metrics import accuracy_score\n","\n","# splitting the train and test dataset \n","train,val = train_test_split(train_df,test_size=0.1)\n","\n","# instantiating dataset class with supplied data and labels \n","t_dataset = GPTDataset(train[\"description\"].values,train[mlb.classes_].values)\n","# dataset and batchwise loader from pytorch \n","t_loader = DataLoader(t_dataset,batch_size=16,shuffle=True,num_workers=4)\n","\n","v_dataset = GPTDataset(val[\"description\"].values,val[mlb.classes_].values)\n","v_loader = DataLoader(v_dataset,batch_size=16,shuffle=False,num_workers=4)\n","\n","model = GPTModel(768,100,len(mlb.classes_))\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#optimizer methodoly used with very low learning rate \n","optimizer = torch.optim.Adam(model.parameters(),lr=1e-5)\n","# loadin the model to device for getting the weights to device \n","model.to(device) \n","best_loss = np.inf\n","print(\"TRAINING STARTED...\")\n","\n","for e in range(epochs):\n","  t_out,t_target,train_loss = train_fn(t_loader,model,optimizer,device)\n","  v_out,v_target,val_loss = validation_fn(v_loader,model,device)  \n","  # here evaluation results from the given file \n","  acc,f1,fpr,fnr = evaluate_results(np.array(t_target),np.round(t_out))\n","  acc2,f1_2,fpr2,fnr2 = evaluate_results(np.array(v_target),np.round(v_out))\n","  print(f\"{e+1}-\")\n","  if val_loss < best_loss:\n","    torch.save(model.state_dict(),\"model_gpt_v2.pth\")\n","    best_loss = val_loss\n","  #uncomment below code to get the accuracy score directly from sklearn \n","  #print((accuracy_score(np.array(t_target),np.round(t_out)),accuracy_score(np.array(v_target),np.round(v_out))))\n","  print(\"train_loss:\", round(train_loss,3),\"train_accuracy:\",round(acc,3), \"train_f1:\", f1, \"train_fpr:\", fpr, \"train_fnr:\", fnr) \n","  print(\"val_loss:\",round(val_loss,3),\"val_accuracy:\",round(acc2,3), \"val_f1:\", f1_2, \"val_fpr:\", fpr2, \"val_fnr:\", fnr2)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"370971a7ab2540ec802652b374834fc5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"19c6b3cdeb674380b4fa56693b4b7ca7","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a0188906e4434ffd9fa59eacc1d1df95","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=665.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6332cfa1bc4c4b79864da45dc0e0a08a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=548118077.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["TRAINING STARTED...\n","1-\n","train_loss: 0.612 train_accuracy: 0.018 train_f1: [0.07692307692307693, 0.0, 0.12981744421906694, 0.18225039619651348, 0.011753183153770812, 0.0, 0.5046931407942239, 0.05726872246696035, 0.0846153846153846, 0.0, 0.10460251046025106, 0.04682274247491639] train_fpr: [0.05587510271158587, 0.005594927265945543, 0.0897308075772682, 0.3158119658119658, 0.011409013120365089, 0.0, 0.5007374631268436, 0.043532338308457715, 0.07900122799836266, 0.0, 0.11662817551963048, 0.043241137514608494] train_fnr: [0.9426751592356688, 1.0, 0.9137466307277629, 0.7181372549019608, 0.9939698492462311, 1.0, 0.4978448275862069, 0.9613095238095238, 0.9278688524590164, 1.0, 0.8333333333333334, 0.9613259668508287]\n","val_loss: 0.532 val_accuracy: 0.0 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","2-\n","train_loss: 0.481 train_accuracy: 0.003 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2275711159737418, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.168141592920354, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8505747126436781, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.425 val_accuracy: 0.007 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1297297297297297, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05673758865248227, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9272727272727272, 1.0, 1.0, 1.0, 1.0, 1.0]\n","3-\n","train_loss: 0.405 train_accuracy: 0.01 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40177383592017746, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3023598820058997, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6745689655172413, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.386 val_accuracy: 0.01 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773809523809523, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.524822695035461, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4121212121212121, 1.0, 1.0, 1.0, 1.0, 1.0]\n","4-\n","train_loss: 0.38 train_accuracy: 0.018 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.545271629778672, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5730088495575221, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.41594827586206895, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.376 val_accuracy: 0.013 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6041055718475073, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5177304964539007, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.37575757575757573, 1.0, 1.0, 1.0, 1.0, 1.0]\n","5-\n","train_loss: 0.371 train_accuracy: 0.022 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5973729297544261, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7846607669616519, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.24856321839080459, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.371 val_accuracy: 0.026 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7006369426751593, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","6-\n","train_loss: 0.368 train_accuracy: 0.027 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6566556375668109, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9196165191740413, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.07327586206896551, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.37 val_accuracy: 0.026 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7006369426751593, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","7-\n","train_loss: 0.366 train_accuracy: 0.022 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6034782608695652, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.2521551724137931, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.369 val_accuracy: 0.023 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6984815618221258, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9574468085106383, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.024242424242424242, 1.0, 1.0, 1.0, 1.0, 1.0]\n","8-\n","train_loss: 0.365 train_accuracy: 0.019 train_f1: [0.0, 0.0, 0.0026917900403768506, 0.0, 0.0020080321285140565, 0.0, 0.6198691255205235, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6843657817109144, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 0.9986522911051213, 1.0, 0.9989949748743718, 1.0, 0.2514367816091954, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.368 val_accuracy: 0.023 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7004405286343613, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9219858156028369, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.03636363636363636, 1.0, 1.0, 1.0, 1.0, 1.0]\n","9-\n","train_loss: 0.362 train_accuracy: 0.013 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6008876749743939, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.48451327433628316, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.367816091954023, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.359 val_accuracy: 0.016 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6686046511627907, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45390070921985815, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.30303030303030304, 1.0, 1.0, 1.0, 1.0, 1.0]\n","10-\n","train_loss: 0.356 train_accuracy: 0.016 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.660795825179387, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.48746312684365783, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.27227011494252873, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.358 val_accuracy: 0.007 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5076923076923078, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20567375886524822, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6, 1.0, 1.0, 1.0, 1.0, 1.0]\n","11-\n","train_loss: 0.35 train_accuracy: 0.015 train_f1: [0.0, 0.0, 0.0, 0.0, 0.0019980019980019984, 0.0, 0.6710439921208142, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.002852253280091272, 0.0, 0.46607669616519176, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 0.9989949748743718, 1.0, 0.26580459770114945, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.353 val_accuracy: 0.007 val_f1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.556338028169014, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28368794326241137, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5212121212121212, 1.0, 1.0, 1.0, 1.0, 1.0]\n","12-\n","train_loss: 0.343 train_accuracy: 0.023 train_f1: [0.0, 0.0, 0.0, 0.0, 0.07692307692307693, 0.0, 0.689655172413793, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.017113519680547633, 0.0, 0.4166666666666667, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 0.9587939698492463, 1.0, 0.2600574712643678, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.342 val_accuracy: 0.036 val_f1: [0.0, 0.0, 0.0, 0.0, 0.22068965517241376, 0.0, 0.6824925816023738, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.06842105263157895, 0.0, 0.40425531914893614, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 0.8620689655172413, 1.0, 0.30303030303030304, 1.0, 1.0, 1.0, 1.0, 1.0]\n","13-\n","train_loss: 0.337 train_accuracy: 0.041 train_f1: [0.0, 0.0, 0.0, 0.0, 0.2854973424449506, 0.0, 0.6970402394413037, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.0764403879064461, 0.0, 0.41814159292035397, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 0.8110552763819096, 1.0, 0.2471264367816092, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.341 val_accuracy: 0.075 val_f1: [0.0, 0.0, 0.0, 0.0, 0.5535714285714285, 0.0, 0.645367412140575, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.24210526315789474, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 0.46551724137931033, 1.0, 0.3878787878787879, 1.0, 1.0, 1.0, 1.0, 1.0]\n","14-\n","train_loss: 0.331 train_accuracy: 0.087 train_f1: [0.0, 0.0, 0.0, 0.0, 0.5488270594653574, 0.0, 0.7034246575342465, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.19110096976611524, 0.0, 0.3694690265486726, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 1.0, 0.49447236180904525, 1.0, 0.26221264367816094, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.339 val_accuracy: 0.078 val_f1: [0.0, 0.0, 0.0, 0.0, 0.5358851674641149, 0.0, 0.6339869281045752, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.19473684210526315, 0.0, 0.3120567375886525, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 1.0, 0.5172413793103449, 1.0, 0.4121212121212121, 1.0, 1.0, 1.0, 1.0, 1.0]\n","15-\n","train_loss: 0.325 train_accuracy: 0.108 train_f1: [0.0, 0.0, 0.0, 0.1111111111111111, 0.6245136186770428, 0.0, 0.7046956521739131, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.0, 0.2390188248716486, 0.0, 0.3466076696165192, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 0.9411764705882353, 0.35477386934673366, 1.0, 0.27227011494252873, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.331 val_accuracy: 0.098 val_f1: [0.0, 0.0, 0.0, 0.18181818181818182, 0.6083333333333334, 0.0, 0.6765578635014837, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0, 0.26842105263157895, 0.0, 0.41134751773049644, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 0.9, 0.3706896551724138, 1.0, 0.3090909090909091, 1.0, 1.0, 1.0, 1.0, 1.0]\n","16-\n","train_loss: 0.319 train_accuracy: 0.118 train_f1: [0.0, 0.0, 0.0, 0.2803347280334728, 0.6294117647058824, 0.0, 0.7115044247787611, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.001282051282051282, 0.22989161437535655, 0.0, 0.31563421828908556, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 0.8357843137254902, 0.35477386934673366, 1.0, 0.27801724137931033, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.338 val_accuracy: 0.101 val_f1: [0.0, 0.0, 0.0, 0.65, 0.46874999999999994, 0.0, 0.5714285714285715, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.015625, 0.1631578947368421, 0.0, 0.24822695035460993, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 0.48, 0.6120689655172413, 1.0, 0.5151515151515151, 1.0, 1.0, 1.0, 1.0, 1.0]\n","17-\n","train_loss: 0.315 train_accuracy: 0.143 train_f1: [0.0, 0.0, 0.0, 0.5574912891986064, 0.6626617375231053, 0.0, 0.7233748271092669, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.002564102564102564, 0.257843696520251, 0.0, 0.33480825958702065, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 0.6078431372549019, 0.2793969849246231, 1.0, 0.24856321839080459, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.326 val_accuracy: 0.127 val_f1: [0.0, 0.0, 0.0, 0.6233766233766234, 0.6337448559670782, 0.0, 0.6624605678233438, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.01171875, 0.2631578947368421, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 0.52, 0.33620689655172414, 1.0, 0.36363636363636365, 1.0, 1.0, 1.0, 1.0, 1.0]\n","18-\n","train_loss: 0.311 train_accuracy: 0.16 train_f1: [0.0, 0.0, 0.0, 0.6964285714285713, 0.6593204775022957, 0.0, 0.7293787293787294, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.01282051282051282, 0.2652595550484883, 0.0, 0.308259587020649, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 0.4264705882352941, 0.27839195979899495, 1.0, 0.2535919540229885, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.326 val_accuracy: 0.111 val_f1: [0.0, 0.0, 0.0, 0.5555555555555556, 0.5607476635514018, 0.0, 0.5752508361204013, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0078125, 0.2, 0.0, 0.3404255319148936, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 0.6, 0.4827586206896552, 1.0, 0.47878787878787876, 1.0, 1.0, 1.0, 1.0, 1.0]\n","19-\n","train_loss: 0.304 train_accuracy: 0.161 train_f1: [0.0, 0.0, 0.0, 0.6810477657935284, 0.6697848456501403, 0.0, 0.729137199434229, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.008547008547008548, 0.24358243011979464, 0.0, 0.29867256637168144, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 0.4583333333333333, 0.2804020100502513, 1.0, 0.2593390804597701, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.327 val_accuracy: 0.121 val_f1: [0.0, 0.0, 0.0, 0.6896551724137931, 0.5581395348837209, 0.0, 0.6020066889632107, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.02734375, 0.20526315789473684, 0.0, 0.3120567375886525, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 0.4, 0.4827586206896552, 1.0, 0.45454545454545453, 1.0, 1.0, 1.0, 1.0, 1.0]\n","20-\n","train_loss: 0.3 train_accuracy: 0.174 train_f1: [0.0, 0.0, 0.0, 0.7597765363128491, 0.6839666357738647, 0.0, 0.725545390570021, 0.0, 0.0, 0.0, 0.0, 0.0] train_fpr: [0.0, 0.0, 0.0, 0.015384615384615385, 0.24244152880775813, 0.0, 0.3089970501474926, 0.0, 0.0, 0.0, 0.0, 0.0] train_fnr: [1.0, 1.0, 1.0, 0.3333333333333333, 0.25829145728643216, 1.0, 0.2593390804597701, 1.0, 1.0, 1.0, 1.0, 1.0]\n","val_loss: 0.325 val_accuracy: 0.118 val_f1: [0.0, 0.0, 0.0, 0.6391752577319586, 0.5882352941176471, 0.0, 0.5906040268456376, 0.0, 0.0, 0.0, 0.0, 0.0] val_fpr: [0.0, 0.0, 0.0, 0.0625, 0.21052631578947367, 0.0, 0.3191489361702128, 0.0, 0.0, 0.0, 0.0, 0.0] val_fnr: [1.0, 1.0, 1.0, 0.38, 0.4396551724137931, 1.0, 0.4666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VCr7IdWGcwQO"},"source":["**Inference - Model3**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0oD2bdiec1Op","executionInfo":{"status":"ok","timestamp":1611658359978,"user_tz":-60,"elapsed":65230,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"8da29c6a-6598-4473-eb41-ff7626b1c4db"},"source":["from transformers import GPT2Tokenizer\n","\n","class GPTTestDataset(Dataset):\n","  \"\"\" This is dataset module for gpt and \n","      everything is similar to the earliar dataset modules seen. \n","      Except the tokenizer and token_ids are not used \n","      Tokenizer used here is GPT2  \n","  \"\"\"\n","  def __init__(self,description):\n","    self.description = description\n","    self.max_seq = 250\n","    self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","  \n","  def __len__(self):\n","    return len(self.description)\n","  \n","  def __getitem__(self,idx): \n","    description = \"\".join(self.description[idx].split()) \n","    inputs = self.tokenizer(description, add_special_tokens=True,truncation=True,max_length=self.max_seq)\n","    input_ids = inputs[\"input_ids\"]\n","    token_type_ids = [0,] \n","    attention_mask = inputs[\"attention_mask\"]\n","    input_ids = input_ids + [0] * (self.max_seq - len(input_ids))\n","    token_type_ids = token_type_ids + [0] * (self.max_seq - len(token_type_ids))\n","    attention_mask = attention_mask + [0] * (self.max_seq - len(attention_mask))\n","    return {\n","        \"input_ids\": torch.tensor(input_ids,dtype=torch.long),\n","        \"token_type_ids\": torch.tensor(token_type_ids,dtype=torch.long),\n","        \"attention_mask\": torch.tensor(attention_mask,dtype=torch.long),\n","    } \n","#test = pd.read_csv(\"dataset/test.csv\") \n","test_dataset = GPTTestDataset(test_df[\"description\"].values)\n","test_loader = DataLoader(test_dataset,batch_size=32,shuffle=False,num_workers=4)\n","#Instantiating the cnn-bert model with arguments \n","model = GPTModel(768,100,len(mlb.classes_))\n","model.load_state_dict(torch.load(\"model_gpt_v2.pth\"))\n","model.to(device) \n","test_results = test_fn(test_loader,model,device) \n","\n","# this function will take list of sigmoid values as input, round it to 1/0\n","# gets the indices of value 1 or get the index of maximum sigmoid value and return corresponding labels.\n","def get_labels(result_labels):\n","  result_indices = np.where(np.round(result_labels)==1.0)[1]\n","  if len(result_indices)==0:\n","    genre = mlb.classes_[np.argmax(result_labels)]\n","    return genre\n","  else:\n","    genres = mlb.classes_[np.where(np.round(result_labels)==1.0)[1]]\n","    return genres\n","\n","test_df[\"genres\"] = test_results\n","test_df[\"genres\"] = test_df[\"genres\"].apply(lambda x: get_labels([x]))\n","test_df.to_csv(\"pred_Model3_uni_f.csv\",index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"OYOoJ-KKc1jP"},"source":["**Prediction - Model3**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hAgpCK-qc50j","executionInfo":{"status":"ok","timestamp":1611657497911,"user_tz":-60,"elapsed":65576,"user":{"displayName":"Amit Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiNa_xErotWLMi2GMX2uQAjHxVtIO8pdWC5MlX5dQ=s64","userId":"12909997751845113123"}},"outputId":"323bf55a-35cd-4622-fd23-1b7502722aca"},"source":["# get the description and title as string  \n","description = \"Banished to a wasteland of undesirables, a young woman struggles to find her feet among a drug-soaked desert society and an enclave of cannibals.\"\n","\n","test = GPTTestDataset([description,])\n","test_loader = DataLoader(test,batch_size=1,shuffle=False)\n","result_labels = test_fn(test_loader,model,device)\n","# show the results from the labels-names, which are rounded to 1\n","result_indices = np.where(np.round(result_labels)==1.0)[1]\n","if len(result_indices)==0:\n","  print(mlb.classes_[np.argmax(result_labels)])\n","else:\n","  print(mlb.classes_[np.where(np.round(result_labels)==1.0)[1]])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Drama' 'International']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_spw66sHfzAt"},"source":["### Conclusion"]},{"cell_type":"markdown","metadata":{"id":"CN4nrsJtf4AW"},"source":["Overall deep learning models didn't perform well on the given dataset. This could be attributed to the fact that the dataset size was pretty small.\n","\n","Also, Model 3 based on accuracy performed better compared to other models. This is possibly because GPT-2 tokenizer is pre-trained on vast set of resources while BERT tokenizer is limited to Wikipedia only."]},{"cell_type":"code","metadata":{"id":"imCeudbUgmhY"},"source":[""],"execution_count":null,"outputs":[]}]}